\section{METG (50\%)}
\label{sec:metg}

\input{f1_flops_mpi}
\input{f2_efficiency_mpi}

The \emph{minimum effective task granularity}, METG(50\%), for an application $A$ is
the smallest average task granularity (i.e., task duration) such that $A$
achieves overall efficiency exceeding 50\%. For example, in
a compute-bound application efficiency might be measured as the
percentage of the available FLOPS achieved. On a machine with a peak
capability of 1.26 TFLOPS per node, METG(50\%) would correspond to
the smallest task granularity achieved while maintaining at least 0.63
TFLOPS. Relative parallel efficiency (vs a sufficiently large problem
size) can also be used in cases where the application isn't easily
characterized by peak resource usage.

Figures~\ref{fig:flops-mpi} and \ref{fig:efficiency-mpi} show how METG is
calculated. The application, in this case an MPI implementation of the
Task Bench stencil pattern in Figure~\ref{fig:task-graphs-stencil}, is
run on a single Haswell node of Cori with a problem size large enough to
ensure that it achieves peak FLOPS. This starting point confirms
that the application is properly configured and that the choice of
efficiency metric is achievable. The problem
size is then repeatedly reduced while maintaining exactly the same hardware
configuration (in particular, the same number of nodes). The
expectation is that as problem size shrinks,
performance will begin to drop and eventually approach zero, as shown in Figure~\ref{fig:flops-mpi}. Systems
with lower runtime overheads maintain higher performance at smaller
problem sizes compared to systems with higher overheads.

To calculate METG, the data is replotted along axes of efficiency
(i.e., as a percentage of the peak FLOPS achieved) and task
granularity (i.e., $\text{wall time} \times \text{number of
  cores}/\text{number of tasks}$), as shown in Figure~\ref{fig:efficiency-mpi}. Note that a \emph{task} is defined
broadly to be any continuously-executing unit of application code,
and thus it makes sense to discuss tasks even in systems
without an explicit notion of tasking, such as in MPI programs that
are written in a bulk synchronous style. In this case, the tasks run a
compute-bound kernel included in the Task Bench implementation,
described in more detail in Section~\ref{sec:task-bench}.

In Figure~\ref{fig:efficiency-mpi}, we see that
efficiency starts near 100\%, and that initially task granularity
drops quickly with minimal change in efficiency. However as task
granularity shrinks further, efficiency begins to drop as well, leading
eventually to a plateau in task granularity. Intuitively, this plateau
represents the fundamental baseline overhead of the system, the point
below which the system simply cannot execute tasks efficiently.

METG(50\%) is the intersection of this curve with a vertical line through
Figure~\ref{fig:efficiency-mpi} at 50\% efficiency. In the figures, the red, dashed line shows the 50\% efficiency
level. At 50\% efficiency, MPI achieves an average task granularity of
4.6 us, thus the METG(50\%) of MPI is 4.6 us for this configuration of
the application. We use 50\% because it is a reasonable level of
efficiency in practice. Lower values would likely not be reasonable,
because operators of computing facilities prefer higher levels of
utilization. Values above 90\% can misrepresent the performance of
some systems (see
Section~\ref{subsec:peak-performance-and-efficiency}).

METG is an appealing metric in part because it has a well-defined
relationship with quantities of interest to application developers,
namely weak and strong scaling. Figures~\ref{fig:weak-scaling-mpi} and
\ref{fig:strong-scaling-mpi} show the behavior of the MPI Task Bench stencil's weak and strong
scaling, respectively, at a variety of problem sizes. Intuitively, at
larger problem sizes MPI is perfectly efficient. This can be seen at
the top of each figure, with flat lines when weak scaling and
ideally-sloped downward lines when strong scaling. Inefficiency begins
to show at smaller problem sizes, towards the bottom of the graph,
where lines become more compressed. At the
very bottom, the lines compress together as running time becomes dominated by overhead. The shape of the curve at the bottom is the same
for both weak and strong scaling; this is the shape of
the METG curve.

\input{f3_weak_scaling_mpi}
\input{f4_strong_scaling_mpi}

METG therefore has a direct relationship with the smallest problem
size that can be weak scaled to a given node count with a given level
of efficiency. With any smaller problem size, 
the runtime overhead begins to dominate useful
computation. Similarly, METG corresponds to the point at which strong
scaling can be expected to stop; as strong scaling runs start with a
larger problem size and progressively shrink it until overheads begin
to dwarf the gains due to continued scaling. In
Figures~\ref{fig:weak-scaling-mpi} and \ref{fig:strong-scaling-mpi},
the dashed, bright red line show the value of METG(50\%) at each node
count.

The METG metric has another useful property. Because METG is measured ``in place'' (i.e.,
without changing the number of nodes or cores available to the
application), METG isolates effects that are
due to shrinking problem size from effects that are due to
increased communication and other resource issues as
progressively larger portions of the machine are used.

Section~\ref{sec:evaluation} contains a comprehensive evaluation of
METG including how it changes with node count for a wide variety of
programming systems.
