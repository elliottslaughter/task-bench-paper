\section{METG}
\label{sec:metg}

\input{f1_flops_mpi}
\input{f2_efficiency_mpi}

The \emph{minimum effective task granularity}, METG(50\%), for an application $A$ is
the smallest average task granularity (i.e., task duration) such that $A$
achieves overall efficiency exceeding 50\%. For example, in
a compute-bound application efficiency might be measured as the
percentage of the available FLOPS achieved. On a machine with a peak
capability of 1.26 TFLOPS per node, METG(50\%) would correspond to
the smallest task granularity achieved while maintaining at least 0.63
TFLOPS. Absolute (rather than relative) efficiency is preferred
because it makes it easier to conduct fair comparisons with
programming systems that reserve one or more cores for internal use.

Figures~\ref{fig:flops-mpi} and \ref{fig:efficiency-mpi} show how METG is
calculated. The application, in this case an MPI implementation of the
Task Bench stencil pattern in Figure~\ref{fig:task-graphs-stencil}, is
run on a single Haswell node of Cori with a problem size large enough to
ensure that it achieves peak FLOPS. This starting point confirms
that the application is properly configured and that the choice of
efficiency metric is achievable. The problem
size is then repeatedly reduced while maintaining exactly the same hardware
configuration (in particular, the same number of nodes). The
expectation is that as problem size shrinks,
performance will begin to drop and eventually approach zero, as shown in Figure~\ref{fig:flops-mpi}. Systems
with lower runtime overheads maintain higher performance at smaller
problem sizes compared to systems with higher overheads.

To calculate METG, the data is replotted along axes of efficiency
(i.e., as a percentage of the peak FLOPS achieved) and task
granularity (i.e., $\text{wall time} \times \text{number of
  cores}/\text{number of tasks}$), as shown in Figure~\ref{fig:efficiency-mpi}. Note that a \emph{task} is defined
broadly to be any continuously-executing unit of application code,
and thus it makes sense to discuss tasks even in systems
without an explicit notion of tasking, such as in MPI programs that
are written in a bulk synchronous style. In this case, the tasks run a
compute-bound kernel included in the Task Bench implementation,
described in more detail in Section~\ref{sec:task-bench}.

In Figure~\ref{fig:efficiency-mpi}, we see that
efficiency starts near 100\%, and that initially task granularity
drops quickly with minimal change in efficiency. However as task
granularity shrinks further, efficiency begins to drop more rapidly, leading
eventually to asymptote. Intuitively, this asymptote
represents the fundamental baseline overhead of the system, the point
below which the system simply cannot execute tasks efficiently.

% FIXME: keep this up-to-date with the graphs

METG(50\%) is the intersection of this curve with 50\% efficiency, as
shown by the red lines in Figure~\ref{fig:flops-mpi} and
\ref{fig:efficiency-mpi}. At 50\% efficiency, MPI achieves an average
task granularity of
4.6 us, thus the METG(50\%) of MPI is 4.6 us for this configuration of
Task Bench. We use 50\% because that is generally an acceptable level
of efficiency in practice, and values above 90\% can misrepresent the
performance of some systems (see
Section~\ref{subsec:peak-performance-and-efficiency}).

METG has a well-defined
relationship with quantities of interest to application developers,
namely weak and strong scaling. Figures~\ref{fig:weak-scaling-mpi} and
\ref{fig:strong-scaling-mpi} show the behavior of the MPI Task Bench stencil's weak and strong
scaling, respectively, at a variety of problem sizes. In these
figures, the vertical axis is shown as wall time to emphasize the
relationship to time-to-solution, but it could equivalently be shown
as task granularity (as the number of tasks per execution is
fixed). Intuitively, at
larger problem sizes MPI is perfectly efficient. This can be seen at
the top of each figure, with flat lines when weak scaling and
ideally-sloped downward lines when strong scaling. Inefficiency begins
to appear at smaller problem sizes, towards the bottom of the graph,
where lines become more compressed. At the
very bottom, the lines compress together as running time becomes dominated by overhead. The shape of the curve at the bottom is the same
for both weak and strong scaling; this is
the METG curve.

\input{f3_weak_scaling_mpi}
\input{f4_strong_scaling_mpi}

METG therefore has a direct relationship with the smallest problem
size that can be weak scaled to a given node count with a given level
of efficiency. With smaller problem sizes runtime overhead increasingly dominates useful
computation. Similarly, METG corresponds to the point at which strong
scaling can be expected to stop; as strong scaling runs start with a
larger problem size and progressively shrink it until overheads begin
to dwarf the gains due to continued scaling. In
Figures~\ref{fig:weak-scaling-mpi} and \ref{fig:strong-scaling-mpi},
the dashed, bright red line show the value of METG(50\%) at each node
count.

The METG metric has another useful property. Because METG is measured ``in place'' (i.e.,
without changing the number of nodes or cores available to the
application), METG isolates effects that are
due to shrinking problem size from effects that are due to
increased communication and other resource issues as
progressively larger portions of the machine are used. Section~\ref{sec:evaluation} contains a comprehensive evaluation of
METG including how it changes with node count for a wide variety of
programming systems.
