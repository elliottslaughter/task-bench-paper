\section{METG}
\label{sec:metg}

\input{f1_flops_mpi}
\input{f2_efficiency_mpi}

Minimum effective task granularity (METG) is defined intuitively as
the smallest average task granularity such that the application
achieves overall efficiency exceeding some threshold. For example, in
a compute-bound application efficiency might be measured as the
percentage of the available FLOPS achieved. On a machine with a peak
capability of 1.2 TFLOPS per node, METG(50\%) would correspond to
smallest task granularity achieved while maintaing at least 0.6
TFLOPS. Absolute efficiency (i.e., FLOPS or bytes per second) is
preferrable, but parallel efficiency (relative to a sufficiently large
problem size) can also be used in applications which are not amenable
to being characterized by peak resource utilization.

Figure~\ref{fig:flops-mpi} shows the way in which METG is
calculated. The application is run with a large enough problem size to
ensure that it achieves peak utilization of the machine. This confirms
that the application is properly configured and that the choice of
efficiency metric is achievable. Once this is established, the problem
size is shrunk (in a manner analogous to what happens on a node during
a strong scaling run). The expectation is as problem size shrinks, the
efficiency will begin to drop and eventually approach zero. Systems
with lower overheads will maintain higher efficiency at smaller
problem sizes compared to systems with higher overheads.

To calculate METG, this data is replotted along axes of efficiency
(i.e., as a percentage of the peak FLOPS achieved) and task
granularity (i.e., $\text{wall time} \times \text{number of
  cores}/\text{number of tasks}$). Note that a \emph{task} is defined
broadly to be any continuously-executing unit of work in the
application, thus it makes sense to discuss tasks even in systems
without an explicit notion of tasking, such as in MPI codes written in
a bulk synchronous style.

Figure~\ref{fig:efficiency-mpi}
shows the replotted data from above. In this chart we can see that
efficiency starts near 100\%, and that initially task granularity
drops quickly with minimal change in efficiency. However as task
granularity shrinks, efficiency begins to drop as well, leading
eventually to a plateau in task granularity. Intuitively, this plateau
represents the fundamental baseline overhead of the system, the point
beyond which the system simply cannot execute tasks efficiently.

METG is thus the result of drawing a vertical line through
Figure~\ref{fig:efficiency-mpi} at the desired level of efficiency,
and taking the intersection of this line with the task granularity
curve. In the figures, the red, dashed line shows the 50\% efficiency
level. At 50\% efficiency, MPI achieves an average task granularity of
4.5 us, thus the METG(50\%) of MPI is 4.5 us for this configuration of
the application. The number 50\% is chosen arbitrarily, but is a
reasonable choice for this application because it represents a point
at which many systems will have plateaued, yet the overall efficiency
is still reasonable. Lower values would likely not be reasonable,
because in most cases operators of computing facilities prefer to
maintain higher levels of utilization of the machines. And higher
values risks misrepresenting the performance of some systems (not
shown in the figures above).

METG is an appealing metric in part because it has a well-defined
relationship with quantities of interest to application developers,
namely weak and strong scaling. Figures~\ref{fig:weak-scaling-mpi} and
\ref{fig:strong-scaling-mpi} show the behavior of MPI weak and strong
scaling, respectively, at a variety of problem sizes. Intuitively, at
larger problem sizes MPI is perfectly efficient. This can be seen at
the top of each figurs, with flat lines when weak scaling and
ideally-sloped downward lines when strong scaling. Inefficiency begins
to show up at smaller problem sizes, towards the bottom of the graph,
where lines become more compressed and begin to trend upward. At the
very bottom, the lines compress together as it the fundamental
overhead of MPI dwarfs any gains from running smaller
problems. Critically, the shape of the curve at the bottom is the same
for both weak and strong scaling; this is the precisely the shape of
the METG curve.

\input{f3_weak_scaling_mpi}
\input{f4_strong_scaling_mpi}

METG therefore has a direct relationship with the smallest problem
size that can be weak scaled to a given node count with a given level
of efficiency. Any smaller and the overhead begins to dominate useful
computation. Similarly, METG corresponds to the point at which strong
scaling can be expected to stop; as strong scaling runs start with a
larger problem size and progressively shrink it until overheads begin
to dwarf the gains due to continued scaling. In
Figures~\ref{fig:weak-scaling-mpi} and \ref{fig:strong-scaling-mpi},
the dashed, bright red line show the value of METG(50\%) at each node
count.

The METG metric has one particularly useful property in comparison to
weak and strong scaling. Because METG is measured ``in place'' (i.e.,
without changing the number of nodes or cores available to the
application), it is possible to use METG to isolate effects that are
due to shrinking problem size from effects that are due to e.g.,
increased communication latencies as progressively larger portions of
the machine are being used. In comparison, with only a single strong
or weak scaling run, it is impossible to determine if the shape of the
curve is due to the overhead of the runtime system, or communication
effects as progressively larger portions of the machine are being
used.

Section~\ref{sec:evaluation} contains a comprehensive evaluation of
METG including how it changes with node count for a wide variety of
programming systems.

% talk about TPS / MTG ?
