\section{Implementations}
\label{sec:implementation}

\input{t1_systems}

We have implemented Task Bench in the 15 parallel and distributed
programming systems listed in Table~\ref{tab:systems}. These include
traditional HPC programming models (MPI and MPI+X), PGAS and actor
models (Chapel, Charm++ and X10), task-based systems (OmpSs, OpenMP
4.0, PaRSEC, Realm, Regent, and StarPU) and systems for large scale
data analytics, machine learning and workflows (Dask, Spark, Swift/T,
and TensorFlow). We briefly describe the systems and implementations
below.

One challenge in targeting such a wide variety of
systems is that the capabilities of the systems vary considerably. For
example, some systems are \emph{implicitly parallel}, and provide some
form of parallelism discovery from sequential programs, whereas others
are \emph{explicitly parallel} and require users to specify the
parallelism in the program. For systems that provide both implicit and explicit parallelism, the form of parallelism used in Task Bench is emphasized in Table~\ref{tab:systems}.

%% The Task
%% Bench implementations are intended to reflect how actual applications
%% would be written in the respective systems. Places where the Task
%% Bench implementations may be unidiomatic are noted below.

%% There are also aspects of some systems that we do not consider. For
%% example, tasks may run on CPUs, GPUs, or any execution resource
%% supported by a programming system. However, the level of support for
%% heterogeneous processors varies widely across programming systems (if it exists at all), so
%% in order to include as many systems as possible in our comparisons, we
%% evaluate Task Bench only on CPUs in this paper.

In all cases, members of the programming systems' teams
were consulted in the development and evaluation of the
corresponding Task Bench implementations. Where assistance was provided, the insights helped ensure that we provide the highest quality implementations for each system.

\subsection{Traditional HPC Programming Models}

MPI~\cite{MPI} is a message-passing API for HPC. We provide an MPI
implementation written in the style of communicating sequential
processes (CSP). An excerpt of this implementation is shown in
Listing~\ref{lst:code-sample}.

We provide two MPI+X implementations to evaluate hierarchical
programming models. Our MPI+OpenMP implementation uses forall-style
parallel loops to execute tasks, but otherwise follows the CSP
implementation above. The code uses shared memory for data movement
within a rank. Our MPI+CUDA implementation follows an offload model
where data is copied to and from the GPU on every timestep.

\subsection{PGAS and Actor Models}

PGAS and actor models, such as Chapel~\cite{Chapel15},
Charm++~\cite{Charmpp93} and X10~\cite{X1005}, provide support for
asynchronous tasks that make them amenable to a straightforward
implementation of task bench. Synchronization is explicit and may be
provided by messages (in actor models) or other primitives, such as
locks or atomics (in PGAS models). PGAS models such as Chapel and X10
provide global references to data anywhere in the machine, but vary in
whether data can be accessed remotely or not (Chapel allows this, X10
does not).

Chapel also provides support for implicit parallelism which we do not
evaluate in this paper.

\subsection{Task-Based Programming Models}

Task-based systems include OmpSs~\cite{OmpSs11}, OpenMP
4.0~\cite{OpenMPSpec40}, PaRSEC~\cite{PARSEC13, PARSEC_DTD},
Realm~\cite{Realm14}, Regent~\cite{Regent15}, and
StarPU~\cite{StarPU11}. Though the details vary, these systems
typically provide implicit parallelism, where tasks are enumerated
sequentially (in program order) and the dependencies between tasks are
analyzed automatically to construct a \emph{dependence graph} that
guides the execution of tasks. PaRSEC provides two modes,
\emph{dynamic task discovery} (DTD)~\cite{PARSEC_DTD}, which operates
as above, and \emph{parameterized task graphs} (PTG)~\cite{PARSEC13},
where the dependence graph is constructed automatically from an
analytical representation of the task graph. Realm (unlike the others
above) is explicitly parallel and requires tasks to be connected
explicitly via \emph{events}.

Among the distributed task-based systems, one question is how to
resolve the inherent scalability bottleneck posed by enumerating tasks
sequentially. PaRSEC and StarPU resolve this by permitting the user to
omit tasks that are not directly connected in the dependence
graph. The program is replicated (for efficiency of the analysis), but
each task is executed only once. Because the dynamic checks to see if
a task should be executed are not free of cost, we also provide a
PaRSEC implementation (under the label \lstinline{shard}) which has
been hand-written to avoid all such checks. Regent performs an
equivalent optimization at compile time~\cite{ControlReplication17}
that does not require user intervention.

\subsection{Data Analytics, Machine Learning and Workflows}

Dask~\cite{Dask15}, Spark~\cite{Spark10} and
TensorFlow~\cite{TensorFlow15} are programming models for large scale
data analytics and machine learning. Dask and TensorFlow provide
domain-specific abstractions built on top of task-based runtimes. Our
implementations directly create tasks and are similar to other
task-based systems above.

Spark provides support for functional operators that implicitly map to
tasks. We use \lstinline[language=Scala]{flatMap} and
\lstinline[language=Scala]{groupByKey} to generate dependencies, and
then \lstinline[language=Scala]{mapPartitions} to execute a series of
tasks. An explicit hash partitioner ensures the correct task
granularity.

Swift/T~\cite{Wozniak13, Armstrong14} is a parallel scripting language
with dataflow semantics, used primarily for workflow automation. Our
implementation is straightforward.
