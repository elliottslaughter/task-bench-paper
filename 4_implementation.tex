\section{Implementations}
\label{sec:implementation}

\input{t1_systems}

We have implemented Task Bench in the 13 parallel and distributed
programming systems listed in Table~\ref{tab:systems}. We describe the
systems, and any salient details of their Task Bench implementations,
below.

One challenge in implementing Task Bench for such a wide variety of
systems is that the capabilities of the systems vary considerably. For
example, some systems are \emph{implicitly parallel}, and provide some
form of parallelism discovery from sequential programs, whereas others
are \emph{explicitly parallel} and require users to specify the
parallelism in the program. As a rule of thumb, we aim for the Task
Bench implementations to be representative of how actual applications
would be written in the respective systems. Places, where the Task
Bench implementations may be unidiomatic, are noted below.

There are also aspects of some systems that we do not consider in our
evaluation. For example, a number of systems provide some level of
support for heterogeneous processors such as GPUs, but the support
varies so widely (if it exists at all) that it is challenging to come
up with a representative benchmark which allows these features to be
compared in an apples-to-apples manner.

In all cases, members of the respective programming systems' teams
were consulted during the development and evaluation of the
corresponding Task Bench implementations. Their insight was invaluable
to ensure that we provide the highest level of quality in the Task
Bench implementations for each system.

\subsection{Chapel}

Chapel~\cite{Chapel07} is an explicitly parallel programming language
that follows a \emph{multi-resolution} approach to parallel
programming, with support for parallelism at a variety of
levels. Chapel's core features are a partitioned global address space
(PGAS) which permits remote access to objects from anywhere in the
machine, a data model for the distribution of data structures over the
global address space via \emph{domain maps}, tasks, and
synchronization primitives. Chapel thus permits a great deal of
flexibility in the choice of implementation strategy for applications.

For Task Bench, we decided to target a relatively low-level
implementation in Chapel, using explicit task instantiation (via
\lstinline[language=Chapel]{coforall}), bulk access to distributed
arrays for efficient data movement, and atomic integers for
synchronization. The advantage of this approach is that it permits
fine-tuning of the implementation for performance, and avoids relying
on advanced optimizations in the Chapel compiler, which might struggle
with the dynamic, input-dependent nature of the task graphs in Task
Bench.

Chapel also supports a more high-level approach to parallel
programming in which the granularities of tasks are determined
automatically by the compiler (via
\lstinline[language=Chapel]{forall}). This is the level that users
would typically target in their programs. The specific dependence
patterns supported by Task Bench might also be implementable at this
level, though a full Task Bench implementation would be challenging.

% FIXME: Does the Chapel compiler handle: bulk data movement, point-to-point synchronization

Internally, Chapel implements custom support for Cray networks, and
uses the GASNet~\cite{GASNET07} compatibility layer on other networks.

\subsection{Charm++}

Charm++~\cite{Charmpp93} is an explicitly parallel programming system
based on the actor model of distributed computation. Actors, or
\emph{chares}, are objects that live in their own address space and
can be migrated automatically between nodes for better load
balance. Chares are permitted to send messages to other chares via a
form of remote procedure call (RPC). Calling a method of another chare
is the primary form of synchronization and data movement in
Charm++. Additional support is provided for collective operations
between chares.

Our Task Bench implementation is straightforward: we create a chare
array for the task graph, with one chare for each column. Methods are
used to implement dependencies, and a task is executed as soon as its
dependencies are all available. Collectives are used to synchronize the
completion of the task graph execution.

% FIXME: Discuss SMP vs non-SMP versions

Internally, Charm++ implements support for a variety of networks
directly on top of the low-level, vendor-provided APIs. For Cori, we
use Charm++'s native support for uGNI.

\subsection{MPI}

MPI~\cite{MPI} is the industry standard message-passing API for
programming supercomputers, and is the most widely used programming
model in high-performance scientific computing today. The goal of our
MPI implementation of Task Bench is to represent the common (if
unexciting) case which continues to dominate high-performance
computing: bulk synchronous programming with distinct computation and
communication phases. We use \lstinline[language=C++]{MPI_Isend} and
\lstinline[language=C++]{MPI_Irecv} to implement the communication
phase, posting receives before sends. Each task dependency maps to one
send/receive pair in MPI. Despite the relatively straightforward
design, our MPI implementation is among the best performing.

In our evaluation, we use the default MPI implementation installed on
the machine, which is Cray MPI on the Cori supercomputer. This
implementation is vetted by the machine operators and vendor and is
the highest performing available implementation for the hardware.

For completeness, we also explored the use of additional MPI calls:
namely buffered sends (\lstinline[language=C++]{MPI_Bsend}),
broadcasts (\lstinline[language=C++]{MPI_Bcast}), and all-to-all
communication (\lstinline[language=C++]{MPI_Alltoallv}). For the
configurations used in our evaluation, we found non-blocking sends and
receives to be the best performing, often by a wide margin.

\subsection{OmpSs}

OmpSs~\cite{OmpSs11} is a programming model for loop-based parallelism
that is source-compatible with OpenMP. Our Task Bench implementation
uses OpenMP 4.0-style tasks with task dependencies. Because tasks in
OpenMP 4.0 have a fixed number of dependencies, we use a switch
statement to implement the dynamic dependencies required for Task
Bench. The implementation is otherwise straightforward.

% FIXME: More to say about OmpSs? Is there anything different vs the OpenMP implementation?

\subsection{OpenMP}

OpenMP~\cite{OpenMPSpec40} is the industry standard API for loop-based
parallelism on shared-memory systems. Our Task Bench implementation
for OpenMP is very similar to OmpSs and uses OpenMP 4.0 tasks with
task dependencies.

In our evaluation, we test with both GNU GOMP and Intel KMP
implementations of OpenMP. We found KMP to provide better performance
and thus report KMP numbers in our evaluation.

\subsection{PaRSEC}

PaRSEC~\cite{PARSEC13} is a task-based programming system which
supports two distinct programming models: \emph{parameterized task
  graphs} (PTG) and \emph{dynamic task discovery} (DTD)~\cite{PARSEC_DTD}. 
PTG is a data flow model, where programmers write applications 
in a concise description of all data flows by describing tasks 
and how data flows between tasks. This creates a compressed algebraic
representation of the task graph, which is then transformed into C 
code by PaRSEC source-to-source compiler. PTG is therefore a more limited
programming model, but can potentially provide better performance
because the graph description is amenable to static analysis and can
be pruned efficiently for better horizontally scalable execution.

In the DTD model, tasks are enumerated in program order (by
dynamically executing the program), and dependencies between tasks are
identified automatically based on the input and output data of tasks. 
A task depends on another task if it reads data written by the other task,
and the data is copied automatically if the two tasks are executed on
different nodes. While the basic DTD model is implicitly parallel
because the system is responsible for the discovery of parallelism, in
order to achieve horizontal scalability, the program is executed in a
SPMD fashion on all nodes, and the user is responsible for eliding
tasks that are not directly connected to those that are to be executed
on the current node.

The PaRSEC build-in 2D block cyclic matrix is used to store data. 
A matrix is divided into M by N tiles where N equals to the width of
task graphs and M varies from 2 to the height of task graphs depends
on the optimization level of alleviating write-after-read depdenencies. 
Tiles are distributed across nodes in pre-defined patterns, 
and can be passed to tasks on other nodes, but
can only be written on the node where the data is originally
allocated. In our Task Bench implementation, each task writes to a
tile as output, and these tiles are passed as inputs to the
subsequent set of tasks. As in the OmpSs and OpenMP implementations,
the task launch code uses a switch statement as the PaRSEC API assumes
tasks have a fixed number of dependencies.

% FIXME: What does PaRSEC call regions?

Internally, PaRSEC uses MPI for network communication.

\subsection{Realm}

Realm~\cite{Realm14} is an explicitly-parallel task-based programming
model that is used internally by Legion~\cite{Legion12} and
Regent~\cite{Regent15}, which are implicitly parallel layers written
on top of Realm. Thus the Realm implementation of Task Bench can be
seen as a limit study of what is possible with Legion or Regent.

Tasks in Realm are executed in a deferred manner, with dependencies
between tasks explicitly defined by \emph{events} passed from one task
to another. Realm's data model supports regions which live on a single
node. Data must be explicitly copied, and the copies synchronized with
tasks via events.

Realm also supports a subgraph API where sets of tasks and copies are
issued together. This mode permits additional optimizations to be
performed. We have implemented Task Bench in both individual and
subgraph styles, but report numbers for the subgraph version as it
provides better performance.

Internally, Realm uses GASNet~\cite{GASNET07} for communication.

\subsection{Regent}

Regent~\cite{Regent15} is an implicitly-parallel task-based language
which implements the Legion programming model~\cite{Legion12}. We use
Regent rather than Legion directly because the Regent compiler
provides a critical optimization, \emph{control replication}, that
enables superior horizontal scalability of
applications~\cite{ControlReplication17}.

In Regent tasks are executed sequentially and parallelism is
discovered automatically based on the data that is read or written by
tasks. Regent programs are agnostic to the machine architecture and
are separately \emph{mapped} for execution on a distributed memory
system. Copies are inserted automatically by the compiler and runtime
whenever data is read on a different node from where it is written.

The Regent compiler performs control replication to slice the Regent
program into \emph{shards} of execution which can scalably execute on
large numbers of nodes. Our Regent implementation also makes use of a
tracing optimization~\cite{LegionTracing18} which substantially
reduces the cost of dynamic analysis of tasks in Regent.

\subsection{Spark}

Spark~\cite{Spark10} is an implicitly-parallel programming model for
data analytics, widely used in industrial data center applications. As
one of the programming models in this study not originally intended
for scientific computing, Spark makes very different tradeoffs from
other systems under consideration, and therefore makes an interesting
point of comparison.

Spark's programming model is based on \emph{functional parallelism}, a
generalization of the MapReduce programming
model~\cite{MapReduce04}. The core computational abstraction in Spark
are operators such as map, reduce, filter, join, etc. which by virtue
of being pure functions are trivially known to be
parallelizable. Functions in Spark operate on \emph{resilient
  distributed datasets} (RDDs), which are globally-visible,
dynamically single-assignment data structures that cached in memory
and written to disk for durability. One of Spark's primary
optimizations therefore is the caching of RDDs to avoid unnecessary
disk traffic, resulting in substantial speedups over MapReduce.

Though Spark has tasks internally, these are not exposed to the user,
so one of the challenges in developing a Task Bench implementation is
mapping the task graph to a set of operators which will result in the
desired performance behavior. We use a combination of
\lstinline[language=Scala]{flatMap} and
\lstinline[language=Scala]{groupByKey} operations to generate task
dependencies, and then \lstinline[language=Scala]{map} to execute a
series of tasks. We use an explicit hash partitioner to ensure that
Spark does not attempt to group multiple Task Bench tasks into a
single Spark task.

We performed an extensive set of experiments to verify that no
extraneous factors interfered with our Spark measurements. We
completely disabled logging, ensured that no RDDs are actually written
to disk, confirmed that there is no measurable overhead due to JNI
calls from Java to C, and cross-checked our results with known cases
that hit optimal task throughput in Spark, among other things. Our
results are discussed in detail in Section~\ref{sec:evaluation}.

\subsection{StarPU}

StarPU~\cite{StarPU11} is a task-based system that supports an DTD
programming model similar to PaRSEC. Our Task Bench implementation in
StarPU is very similar to PaRSEC, including the use of a switch
statement to handle dynamic numbers of task dependencies, and manual
elision of tasks on unrelated nodes to provide horizontal scalability.
A simplified version of 2D block cyclic matrix from the Chameleon 
project~\cite{Chameleon} is used to describe data of tasks in 
our Task Bench implementation.

Internally, StarPU uses MPI for communication.

\subsection{Swift/T}

Swift/T~\cite{Wozniak13} is a parallel scripting language intended
primarily for the composition of workflows in high-performance
scientific computing. Tasks in Swift/T can be written in any
programming language, and may even be themselves parallel. Swift/T
programs follow dataflow semantics, where every statement may
potentially execute in parallel as soon as its dependencies are
satisfied; statements only execute sequentially when explicitly
requested. The Swift/T compiler performs a number of optimizations to
improve performance of highly parallel programs~\cite{Armstrong14}.

Our Task Bench in Swift/T is straightforward, using Swift/T's dataflow
semantics to capture dependencies on other tasks.

Internally, Swift/T uses MPI for communication.

\subsection{TensorFlow}

TensorFlow~\cite{TensorFlow15} is a programming system intended for
the executing deep learning workloads, widely used in industrial data
center applications. Although TensorFlow's high-level API exposes
concepts specific to machine learning, internally TensorFlow is
implemented as a task graph execution engine, making it a good fit for
Task Bench. TensorFlow programs are written by creating nodes in the
task graph, and then connecting these nodes with edges to implement
the desired dataflow. Task graphs are composed in a high-level
language such as Python, but then serialized and run by an efficient
execution engine written in C++.

Our Task Bench implementation in TensorFlow works by defining a custom
operator, which we instantiate for each point in the task graph. We
then define dataflow edges to correspond to the task dependencies.

\subsection{X10}

X10~\cite{X1005} is an explicitly parallel programming language for
place-based programming. The core features of X10 are \emph{places}
which represent distributed memories, a PGAS model where references to
remote objects can be held (but can only be deferenced on the local
place), asynchronous tasks, and a place-changing construct to move the
execution of a task to a remote place. X10 also supports a variety of
synchronization primitives.

Our Task Bench implementation uses
\lstinline[language=X10]{Rail.asyncCopy} for efficient data movement
between places, along with place-changing and atomic integers for
synchronization. We use the C++ backend of X10, which uses MPI
internally for portability to a variety of networks, and which
conveniently integrates well with the Task Bench API.
