\section{Implementations}
\label{sec:implementation}

\input{t1_systems}

We have implemented Task Bench in the 13 parallel and distributed
programming systems listed in Table~\ref{tab:systems}. We describe the
systems, and any salient details of their Task Bench implementations,
below.

One challenge in implementing Task Bench for such a wide variety of
systems is that the capabilities of the systems vary considerably. For
example, some systems are \emph{implicitly parallel}, and provide some
form of parallelism discovery from sequential programs, whereas others
are \emph{explicitly parallel} and require users to specify the
parallelism in the program. The Task
Bench implementations are intended to reflect how actual applications
would be written in the respective systems. Places where the Task
Bench implementations may be unidiomatic are noted below.

There are also aspects of some systems that we do not consider in our
evaluation. For example, a number of systems provide some level of
support for heterogeneous processors such as GPUs, but the support
varies so widely (if it exists at all) that it is challenging to come
up with a representative benchmark that allows these features to be
compared in an apples-to-apples manner.

In all cases, members of the respective programming systems' teams
were consulted during the development and evaluation of the
corresponding Task Bench implementations. Where assistance was provided, the insights were invaluable
in helping to ensure that we provide the highest level of quality in the Task
Bench implementations for each system.

\subsection{Chapel}

Chapel~\cite{Chapel15} is a programming language
that follows a \emph{multi-res\-o\-lu\-tion} approach to parallel
programming, with support for parallelism at a variety of
levels. Chapel's core features are a partitioned global address space
(PGAS), data distributions, tasks,
synchronization primitives, and array promotion. For Task Bench, we decided to target a low-level implementation in
Chapel, using explicit task instantiation (via
\lstinline[language=Chapel]{coforall}), bulk access to distributed
arrays for efficient data movement, and atomic integers for
synchronization.

\subsection{Charm++}

Charm++~\cite{Charmpp93} is an explicitly parallel programming system
based on the actor model. Actors, or
\emph{chares}, are objects that live in their own address space;
chares communicate data and synchronize via messages, and can be moved
to balance load. Our Task Bench implementation uses a chare
array for the task graph, with one chare for each column of the task graph. Messages are
used to implement dependencies, and a task is executed as soon as its
dependencies are all available. Collectives are used to synchronize the
completion of the task graph execution.

% FIXME: Discuss SMP vs non-SMP versions

\subsection{MPI}

Our MPI~\cite{MPI} implementation of Task Bench represents the common
case of bulk synchronous programs with distinct computation and
communication phases. We experimented with a variety of implementation
strategies and found the best performing to be using
\lstinline[language=C++]{MPI_Isend} and
\lstinline[language=C++]{MPI_Irecv} to implement the communication
phase, posting receives before sends. Each task dependency maps to one
send/receive pair in MPI.

\subsection{OmpSs}

OmpSs~\cite{OmpSs11} is a programming model for loop- and task-based parallelism
that is source-compatible with OpenMP. Our Task Bench implementation
uses OpenMP 4.0-style tasks with task dependencies. Because tasks in
OpenMP 4.0 have a fixed number of dependencies, we use a switch
statement to implement the dynamic dependencies required for Task
Bench. The implementation is otherwise straightforward.

% FIXME: More to say about OmpSs? Is there anything different vs the OpenMP implementation?

\subsection{OpenMP}

OpenMP~\cite{OpenMPSpec40} is the industry standard API for loop-based
parallelism on shared-memory systems, and supports task dependencies as of version 4.0. Our Task Bench implementation
for OpenMP is very similar to OmpSs and uses tasks with
task dependencies.

In our evaluation, we test with both GNU GOMP and Intel KMP
implementations of OpenMP. We found KMP to provide better performance
and thus report KMP numbers in our evaluation.

% FIXME: keep this in sync with experiments

\subsection{PaRSEC}

PaRSEC is a task-based programming system supporting two distinct
programming models: \emph{parameterized task graphs}
(PTG)~\cite{PARSEC13} and \emph{dynamic task discovery}
(DTD)~\cite{PARSEC_DTD}.  PTG is a dataflow model in which programmers
write a concise, algebraic description of the tasks and dataflows in
the program. This compressed representation is expanded into a full
task graph by a source-to-source compiler.

In the DTD model, tasks are enumerated in program order (by
executing the program), and dependencies between tasks are
identified automatically based on the input and output data of tasks. 
A task depends on another task if it reads data written by the other task,
and the data is copied automatically if the two tasks are executed on
different nodes. While the basic DTD model is implicitly parallel
because the system is responsible for the discovery of parallelism, to
achieve horizontal scalability, the program is executed in a
SPMD fashion on all nodes, and the user is responsible for eliding
tasks that are not directly connected to those that are to be executed
on the current node.

The 2D block cyclic is the data distribution used by TaskBench, 
one out of many available in PaRSEC. Tiles of the matrix are 
distributed across nodes in pre-defined patterns,
and can be passed to tasks on other nodes, but
can only be written on the node where the data is originally
allocated. In our Task Bench implementation, each task writes to a
tile as output, and these tiles are passed as inputs to the
subsequent sets of tasks. As in the OmpSs and OpenMP implementations,
the task launch code uses a switch statement as the PaRSEC API assumes
tasks have a fixed number of dependencies.

\subsection{Realm}

Realm~\cite{Realm14} is an explicitly-parallel task-based programming
model used internally by Legion~\cite{Legion12} and
Regent~\cite{Regent15}. Thus the Realm implementation of Task Bench can be
seen as a limit study of what can be achieved with Legion or Regent.

Tasks in Realm are executed in a deferred manner, with dependencies
between tasks explicitly defined by \emph{events} passed from one task
to another. Realm's data model supports collections that live in a
specific memory. Data must be explicitly copied, and the copies
synchronized with tasks via events.

Realm also supports a subgraph API where sets of tasks and copies are
issued together. This mode permits additional optimizations to be
performed. We have implemented Task Bench in both individual and
subgraph styles, but report numbers for the subgraph version as it
provides better performance.

\subsection{Regent}

Regent~\cite{Regent15} is an implicitly-parallel task-based language
which implements the Legion programming model~\cite{Legion12}. We use
Regent rather than Legion directly because the Regent compiler
provides a critical optimization, \emph{control replication}, that
enables superior horizontal scalability~\cite{ControlReplication17}.

\subsection{Spark}

Spark~\cite{Spark10} is an implicitly-parallel programming model for
data analytics, widely used in industrial data center applications. As
one of the programming models in this study not originally intended
for scientific computing, Spark makes very different tradeoffs from
other systems under consideration, and therefore makes an interesting
point of comparison.

The core abstractions in Spark are functional operators such as map,
reduce, filter, join, etc. Functions in Spark operate on
\emph{resilient
  distributed datasets} (RDDs), which are globally-visible,
dynamically single-assignment data structures that are cached in memory
and written to disk for durability. One of Spark's primary
optimizations therefore is the caching of RDDs to avoid unnecessary
disk traffic.

Though Spark has tasks internally, these are not exposed to the user,
so one of the challenges in developing a Task Bench implementation is
mapping the task graph to a set of operators which will result in the
desired performance behavior. We use a combination of
\lstinline[language=Scala]{flatMap} and
\lstinline[language=Scala]{groupByKey} operations to generate task
dependencies, and then \lstinline[language=Scala]{mapPartitions} to execute a
series of tasks. We use an explicit hash partitioner to ensure that
Spark does not attempt to group multiple Task Bench tasks into a
single Spark task.

We performed an extensive set of experiments to verify that no
extraneous factors interfered with our Spark measurements. We
completely disabled logging, ensured that no RDDs are actually written
to disk, confirmed that there is no measurable overhead due to JNI
calls from Java to C, and cross-checked our results with known cases
that hit optimal task throughput in Spark, among other things.

\subsection{StarPU}

StarPU~\cite{StarPU11} is a task-based system that supports a \emph{sequential task flow} (STF)
programming model similar to PaRSEC's DTD. Our Task Bench implementation in
StarPU is very similar to PaRSEC, including the use of a switch
statement to handle dynamic numbers of task dependencies, and manual
elision of tasks on unrelated nodes to provide horizontal scalability.
A simplified version of the 2D block cyclic matrix from the Chameleon 
project~\cite{Chameleon} is used to describe the data produced by tasks in 
our Task Bench implementation.

\subsection{Swift/T}

Swift/T~\cite{Wozniak13} is a parallel scripting language intended
primarily for the composition of workflows in high-performance
scientific computing. Tasks in Swift/T can be written in any
programming language, and may even be themselves parallel. Swift/T
programs follow dataflow semantics, where every statement may
potentially execute in parallel as soon as its dependencies are
satisfied; statements only execute sequentially when explicitly
requested. The Swift/T compiler performs a number of optimizations to
improve performance of highly parallel programs~\cite{Armstrong14}.

Our Task Bench in Swift/T is straightforward, using Swift/T's dataflow
semantics to capture dependencies on other tasks.

\subsection{TensorFlow}

TensorFlow~\cite{TensorFlow15} is a programming system intended for
the executing deep learning workloads, widely used in industrial data
center applications. Although TensorFlow's high-level API exposes
concepts specific to machine learning, internally TensorFlow is
implemented as a task graph execution engine, making it a good fit for
Task Bench. TensorFlow programs are written by creating nodes in the
task graph, and then connecting these nodes with edges to implement
the desired dataflow. Task graphs are composed in a high-level
language such as Python, but then serialized and run by an
execution engine written in C++.

Our Task Bench implementation in TensorFlow works by defining a custom
operator, which we instantiate for each point in the task graph. We
then define dataflow edges to correspond to the task dependencies.

\subsection{X10}

X10~\cite{X1005} is an explicitly parallel programming language for
place-based programming. The core features of X10 are \emph{places}
which represent distributed memories, a PGAS model where references to
remote objects can be held (but can only be dereferenced on the local
place), asynchronous tasks, and a place-changing construct to move the
execution of a task to a remote place. X10 also supports a variety of
synchronization primitives.

Our Task Bench implementation uses
\lstinline[language=X10]{Rail.asyncCopy} for efficient data movement
between places, along with place-changing and atomic integers for
synchronization. We use the native backend of X10, which compiles to
C++.
