\section{Evaluation}
\label{sec:evaluation}

In the following section we include additional experiments performed
with Task Bench.

\input{e5_imbalance}

\subsection{Load Imbalance}

One advantage of systems with asynchronous execution capabilities is
the ability to mitigate load imbalance with little or no additional programmer effort, especially in the presence of
task parallelism. To quantify this effect,
Figure~\ref{fig:efficiency-imbalance} plots task granularity vs
efficiency curves under load imbalance where each task's duration is multiplied by a uniform random variable between [0,~1). The task durations are generated with a deterministic
pseudo random number generator with a consistent seed to ensure
identical task durations for all systems.

The MPI implementation of Task Bench, with its distinct computation and communication phases,
suffers the most under load imbalance. The biggest
difference is at large task granularities, where the imbalance
effectively puts an upper bound on MPI efficiency. At smaller task
granularities the effect shrinks and may even reverse as systems hit
their fundamental limits due to overhead.

The remaining differences are due primarily to
different scheduling behaviors. The execution of 4
simultaneous task graphs only partially mitigates the
load imbalance between tasks. Systems that provide an
additional on-node work stealing capability (such as Chapel with the distrib scheduler) see additional gains in
efficiency at large task granularities. However, the use of
work-stealing queues can also impact throughput at small task
granularities. For example, Chapel's default (non-work-stealing) scheduler outperforms Chapel distrib at very small task granularities. We do not consider Charm++ load balancers because the imbalance is \emph{non-persistent} (i.e., timestep $t$ is uncorrelated with timestep $t+1$). We leave analysis of persistent load imbalance to future work.
