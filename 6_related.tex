\section{Related Work}
\label{sec:related-work}

% Force this paragraph to page break at a hyphen because otherwise
% it's going to be nearly impossible to cut enough space out of this
% paper.
%% \let\oldbrokenpenalty\brokenpenalty
\brokenpenalty=0

Parallel and distributed programming systems are usually
evaluated using proxy-/mini-apps, or
microbenchmarks. Mini-apps are explicitly derived from real
applications and hence have the advantage of bearing some
relationships to the original. This advantage typically does not hold
for microbenchmarks.

\brokenpenalty=\oldbrokenpenalty

Though smaller than full applications, mini-apps can be challenging to
implement to a level of quality sufficient for conducting comparative
studies between programming systems. The largest study we know of,
for the mini-app LULESH~\cite{LULESH13}, compares 7 programming
systems (Chapel, Charm++, CUDA, Lizst, Loci, OpenMP, and MPI), each of
which require a separate, tuned implementation (in contrast to
Task Bench). Other
studies usually lack a comprehensive evaluation, even if multiple
implementations are available:

\begin{itemize}

\item
%The initial paper on PENNANT~\cite{PENNANT} includes an
%implementation supporting MPI/OpenMP/MPI+OpenMP; follow-up papers present an implementation in
%Regent~\cite{Regent15, ControlReplication17, LegionTracing18}.
PENNANT has an initial implementation supporting MPI/OpenMP/MPI+OpenMP
published in~\cite{PENNANT}, and another one in Regent published
in~\cite{Regent15, ControlReplication17, LegionTracing18}.  

\item
One follow-up paper for the mini-app CoMD describes a Chapel
implementation~\cite{CoMDChapel16} (comparison against reference
only). Additional follow-up papers consider aspects of the reference
implementation only~\cite{CoMDLoadImbalance17,
  CoMDThreadedModels14}.

\item
A report on the Mantevo project~\cite{Mantevo09} describes a number of
mini-apps, but only includes self-comparisons based on reference
implementations.

\item
A report on MiniAero~\cite{SandiaReportManyTaskRuntimes15} describes
four implementations of the mini-app, but only includes performance
results for three, and of those three only two can be compared in an
apples-to-apples manner as the last implementation uses a structured
rather than an unstructured mesh. Follow-up papers describe another
implementation in Regent~\cite{Regent15,
  ControlReplication17, LegionTracing18} (comparison against reference
only).

\end{itemize}

Microbenchmarks is easier to implement, but risk being
unrepresentative of real applications. 
PRK Stencil~\cite{PRK14}
contains 2 kernels (that combined fit in under 50 lines of C++)
and a halo exchange on a structured grid; an
evaluation~\cite{PRKRuntimes16} compares MPI, SHMEM, UPC,
Charm++, and Grappa. The NAS benchmark suite~\cite{NAS91, NAS95}
consists mostly of small kernels for dense array computations and has
implementations in OpenMP~\cite{NASOpenMP99}, MPI and
MPI+OpenMP~\cite{NASMPIOpenMP00}, and
Charm++~\cite{NASCharm96}. Neither PRK nor NAS achieves the breadth of
coverage, flexibility, or ease of implementation of Task Bench. We
believe the evidence for ease of implementation is clear; Task Bench
is implemented for more systems than all previous mini-apps and
microbenchmarks. Task Bench is also not a fixed set of benchmarks, and
can be adopted to generate new benchmarks easily.

System-specific benchmarks quantify specific aspects
of system performance, such as MPI communication or collective
latency~\cite{MPPTest99, MPIBench01}. These measurements typically do
not generalize beyond the immediate system they measure.

\textsc{coNCePTuaL}~\cite{Conceptual07} is a domain-specific language
for writing network performance tests. \textsc{coNCePTuaL} and Task
Bench both enable the easy creation of new benchmarks, though
\textsc{coNCePTuaL} does so via scripting whereas Task Bench provides
a set of configurable parameters. \textsc{coNCePTuaL} also targets a
lower level of abstraction, optimized more for testing messaging
layers, whereas Task Bench is closer to application level and
therefore enables comparisons of a broader set of parallel and
distributed programming systems.

Limit studies of task scheduling throughput in various runtime systems
often make additional assumptions. A popular assumption is the use of
trivially parallel tasks~\cite{Canary16, Armstrong14}, which as shown
in Section~\ref{subsec:number-of-dependencies} underestimates (often
substantially) the cost of scheduling a task and can also impact scalability.
