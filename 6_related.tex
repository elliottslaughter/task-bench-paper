\section{Related Work}
\label{sec:related-work}

Parallel and distributed programming systems are most
commonly evaluated using proxy- or mini-apps, or
microbenchmarks. Mini-apps are explicitly derived from larger
applications and therefore have the advantage of bearing some
relationship to the original. This advantage typically does not hold
for microbenchmarks.

Though smaller than full applications, mini-apps can be challenging to
implement to a level of quality sufficient for conducting comparative
studies between programming systems. The largest study we know of,
for the mini-app LULESH~\cite{LULESH13}, compares 7 programming
systems (Chapel, Charm++, CUDA, Lizst, Loci, OpenMP, and MPI), each of
which require a separate, tuned implementation (in contrast to
Task Bench). Other
studies usually lack a comprehensive evaluation, even if multiple
implementations are available:

\begin{itemize}

\item
The initial paper on PENNANT~\cite{PENNANT} includes an
implementation that supports MPI/OpenMP/MPI+OpenMP; follow-up papers present an implementation in
Regent~\cite{Regent15, ControlReplication17, LegionTracing18}.

\item
One follow-up paper for the mini-app CoMD describes a Chapel
implementation~\cite{CoMDChapel16} (comparison against reference
only). Additional follow-up papers consider aspects of the reference
implementation only~\cite{CoMDLoadImbalance17,
  CoMDThreadedModels14}.

\item
A report on the Mantevo project~\cite{Mantevo09} describes a number of
mini-apps, but only includes self-comparisons based on reference
implementations.

\item
A report on MiniAero~\cite{SandiaReportManyTaskRuntimes15} describes
four implementations of the mini-app, but only includes performance
results for three, and of those three only two can be compared in an
apples-to-apples manner as the last implementation uses a structured
rather than an unstructured mesh. Follow-up papers describe an
additional implementation in Regent~\cite{Regent15,
  ControlReplication17, LegionTracing18} (comparison against reference
only).

\end{itemize}

Microbenchmarks can be easier to implement, but risk being
unrepresentative of real applications. PRK Stencil~\cite{PRK14}
contains 2 kernels (which combined fit in under 50 lines of C++)
and a halo exchange on a structured grid; an
evaluation~\cite{PRKRuntimes16} compares MPI variants, SHMEM, UPC,
Charm++, and Grappa. The NAS benchmark suite~\cite{NAS91, NAS95}
consist mostly of small kernels for dense array computations and has
implementations in OpenMP~\cite{NASOpenMP99}, MPI and
MPI+OpenMP~\cite{NASMPIOpenMP00}, and
Charm++~\cite{NASCharm96}. Neither PRK nor NAS achieve the breadth of
coverage, flexibility, or ease of implementation of Task Bench. We
believe the evidence for ease of implementation is clear; Task Bench
is implemented for more systems than all previous mini-apps and
microbenchmarks. Task Bench is also not a fixed set of benchmarks, and
it is even easier to use Task Bench to generate new benchmarks.

System-specific benchmarks quantify specific aspects
of system performance, such as MPI communication or collective
latency~\cite{MPPTest99, MPIBench01}. These measurements typically do
not generalize beyond the immediate system they measure.

\textsc{coNCePTuaL}~\cite{Conceptual07} is a domain-specific language
for writing network performance tests. \textsc{coNCePTuaL} and Task
Bench both enable the easy creation of new benchmarks, though
\textsc{coNCePTuaL} does so via scripting whereas Task Bench provides
a set of configurable parameters. \textsc{coNCePTuaL} also targets a
lower level of abstraction, optimized more for testing messaging
layers, whereas Task Bench is closer to application level and
therefore enables comparisons of a broader set of parallel and
distributed programming systems.

Limit studies of task scheduling throughput in various runtime systems
often make additional assumptions. A popular assumption is the use of
trivially parallel tasks~\cite{Canary16, Armstrong14}, which as shown
in Section~\ref{subsec:number-of-dependencies} underestimates (often
substantially) the cost of scheduling a task and can also impact scalability.
