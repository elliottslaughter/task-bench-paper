\section{Related Work}
\label{sec:related-work}

% Force this paragraph to page break at a hyphen because otherwise
% it's going to be nearly impossible to cut enough space out of this
% paper.
%% \let\oldbrokenpenalty\brokenpenalty
\brokenpenalty=0

Parallel and distributed programming systems are often
evaluated using proxy- or mini-apps, or
microbenchmarks. Mini-apps are explicitly derived from larger
applications and hence have the advantage of bearing some
relationship to the original. This advantage typically does not hold
for microbenchmarks.

\brokenpenalty=\oldbrokenpenalty

Though smaller than full applications, mini-apps can be challenging to
implement to a level of quality sufficient for conducting comparative
studies between programming systems. The largest study we know of,
for the mini-app LULESH~\cite{LULESH13}, compares 7 programming
systems (Chapel, Charm++, CUDA, Lizst, Loci, OpenMP, and MPI), each of
which require a separate, tuned implementation (in contrast to
Task Bench). Other
studies usually lack a comprehensive evaluation, even if multiple
implementations are available:

\begin{itemize}

\item
The PENNANT reference implementation supports
MPI/OpenMP/MPI+OpenMP~\cite{PENNANT}. A follow-up paper presents a
Regent implementation~\cite{Regent15}.

\item
One follow-up paper for the mini-app CoMD describes a Chapel
implementation~\cite{CoMDChapel16} (comparison against reference
only). Additional follow-up papers consider aspects of the reference
implementation only~\cite{CoMDLoadImbalance17,
  CoMDThreadedModels14}.

\item
A report on the Mantevo project~\cite{Mantevo09} describes a number of
mini-apps, but only includes self-comparisons based on reference
implementations.

\item
A report on MiniAero~\cite{SandiaReportManyTaskRuntimes15} describes
four implementations of the mini-app, but only includes performance
results for three, and of those three only two can be compared in an
apples-to-apples manner as the last implementation uses a structured
rather than an unstructured mesh. A follow-up paper describes another
implementation in Regent~\cite{Regent15} (comparison against reference
only).

\end{itemize}

Microbenchmarks can be easier to implement, but do not solve the
asymptotic costs of implementation. PRK Stencil~\cite{PRK14} contains a
2D stencil and is evaluated on implementations in MPI, SHMEM, UPC,
Charm++, and Grappa~\cite{PRKRuntimes16}. The NAS benchmark
suite~\cite{NAS91, NAS95} consists mostly of small kernels for dense
or sparse matrix computations and has implementations in
OpenMP~\cite{NASOpenMP99}, MPI and MPI+OpenMP~\cite{NASMPIOpenMP00},
and Charm++~\cite{NASCharm96}. PRK requires $\mathcal{O}(n)$ effort to
implement (for $n$ systems) by virtue of being only a single
computational pattern, while NAS requires $\mathcal{O}(mn)$ overall
effort (for $m$ patterns and $n$ systems). In contrast, Task Bench
requires $\mathcal{O}(m+n)$ effort and is easily extended to cover new
systems or patterns with $\mathcal{O}(1)$ effort for each additional
system or pattern.

System-specific benchmarks quantify specific aspects
of system performance, such as MPI communication or collective
latency~\cite{MPPTest99, MPIBench01}. These measurements typically do
not generalize beyond the immediate system they measure.

\textsc{coNCePTuaL}~\cite{Conceptual07} is a domain-specific language
for writing network performance tests. \textsc{coNCePTuaL} and Task
Bench both enable the easy creation of new benchmarks, though
\textsc{coNCePTuaL} does so via scripting whereas Task Bench provides
a set of configurable parameters. \textsc{coNCePTuaL} also targets a
lower level of abstraction, optimized more for testing messaging
layers, whereas Task Bench is closer to application level and
therefore enables comparisons of a broader set of parallel and
distributed programming systems.

Limit studies of task scheduling throughput in various runtime systems
often make additional assumptions. A popular assumption is the use of
trivially parallel tasks~\cite{Canary16, Armstrong14}, which as shown
in Section~\ref{subsec:number-of-dependencies} underestimates (often
substantially) the cost of scheduling a task and can also impact scalability.
