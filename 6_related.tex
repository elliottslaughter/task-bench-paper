\section{Related Work}
\label{sec:related-work}

Programming systems for parallel and distributed computing are most
commonly evaluated using proxy- or mini-apps, or
micro\-benchmarks. Mini-apps are explicitly derived from larger
applications and therefore have the advantage of bearing some
relationship to the original. This advantage typically does not hold
for microbenchmarks.

Though smaller than full applications, mini-apps can be challenging to
implement to a level of quality sufficient for conducting comparative
studies between programming systems. The largest study we're aware of,
for the mini-app LULESH~\cite{LULESH13}, compares 7 programming
systems (Chapel, Charm++, CUDA, Lizst, Loci, OpenMP, and MPI). Other
studies usually lack a comprehensive evaluation, even if multiple
implementations are available.

\begin{itemize}

\item
The initial paper on PENNANT~\cite{PENNANT} includes an MPI/OpenMP/MPI+OpenMP
implementation; follow-up papers present an implementation in
Regent~\cite{Regent15, ControlReplication17, LegionTracing18}.

\item
One follow-up paper for the mini-app CoMD describes a Chapel
implementation~\cite{CoMDChapel16} (comparison against reference
only). Additional follow-up papers consider aspects of the reference
implementation only~\cite{CoMDLoadImbalance17,
  CoMDThreadedModels14}.

\item
A report on the Mantevo project~\cite{Mantevo09} describes a number of
mini-apps, but only includes self-comparisons based on reference
implementations.

\item
A report on MiniAero~\cite{SandiaReportManyTaskRuntimes15} describes
four implementations of the mini-app, but only includes performance
results for three, and of those three only two can be compared in an
apples-to-apples manner as the last implementation uses a structured
rather than an unstructured mesh. Follow-up papers describe an
additional implementation in Regent~\cite{Regent15,
  ControlReplication17, LegionTracing18} (comparison against reference
only).

\end{itemize}

Microbenchmarks are often easier to implement, but may not be
representative of real-world applications. From the PRK
suite~\cite{PRK14}, the PRK Stencil code has been reasonably widely
implemented, and a follow-up paper~\cite{PRKRuntimes16} compares MPI
variants, SHMEM, UPC, Charm++, and Grappa. However, PRK Stencil is a
single, simple code involving 2 kernels (which combined fit in under 50 lines
of C++) and a halo exchange on a structured grid. The NAS benchmark suite~\cite{NAS91, NAS95} are also relatively widely studied with
implementations in OpenMP~\cite{NASOpenMP99}, MPI and
MPI+OpenMP~\cite{NASMPIOpenMP00}, and
Charm++~\cite{NASCharm96}. The benchmarks consist mostly of small
kernels for dense array computations. Neither the PRK nor the NAS
benchmark suite achieve the breadth of coverage, flexibility, or ease
of implementation of Task Bench. We believe the evidence for ease of
implementation is clear; Task Bench is implemented for more systems
than all previous mini-apps and microbenchmarks. Task Bench is also
not a fixed set of benchmarks, and it is even easier to use Task Bench
to generate a new benchmark to measure some aspect of system
performance.

System-specific benchmarks have been used to quantify specific aspects
of system performance, such as MPI communication or collective
latency~\cite{MPPTest99, MPIBench01}. These measurements typically do
not generalize beyond the immediate system they measure.

\textsc{coNCePTuaL}~\cite{Conceptual07} is a domain-specific language
for writing network performance tests. \textsc{coNCePTuaL} and Task
Bench both enable the easy creation of new benchmarks though
\textsc{coNCePTuaL} does so via scripting whereas Task Bench provides
a set of configurable parameters. \textsc{coNCePTuaL} also targets a
lower level of abstraction, optimized more for testing messaging
layers, whereas Task Bench is closer to application level and
therefore enables comparisons of a broader set of parallel and
distributed programming systems.

Limit studies of task scheduling throughput in various runtime systems
often make additional assumptions. A popular assumption is the use of
trivially parallel tasks~\cite{Canary16, Armstrong14}, which as shown
in Section~\ref{subsec:number-of-dependencies} underestimates (often
substantially) the cost of scheduling a task, and can also impact scalability.
