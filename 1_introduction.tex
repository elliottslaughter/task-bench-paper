\section{Introduction}
\label{sec:introduction}

The challenge of parallel and distributed computation has led to a
wide variety of proposals for programming models, languages, and
runtime systems. While these systems are well-represented in the literature, comprehensive and comparative performance evaluations
remain difficult to find. Our
goal in this paper is to develop a meaningful and useful framework for
comparing the performance of parallel and distributed programming
systems, to help users and developers evaluate the performance tradeoffs of these systems.

One approach to comparing performance is through \emph{proxy-} or
\emph{mini-apps}. Because they distill key computational characteristics of larger
applications, mini-apps offer insight
without the expense of developing production codes. However, despite the name, our experience is that
mini-apps still require significant investment to develop
to the level of quality needed for useful benchmarking. In many cases,
the effort to tune for performance exceeds the effort to develop a correct implementation. As a result, implementations of mini-apps
often do not reach the level of maturity required to compare
systems. Few published studies compare more than a handful of systems~\cite{LULESH13}.

We present Task Bench, a parameterized benchmark for exploring the performance
of parallel and distributed programming systems under a
variety of conditions.  The key property of Task Bench is that it completely separates
the system-specific implementation from the implementation
of the benchmarks themselves.
In all previous benchmarks we know of, the effort to implement $m$ benchmarks on $n$
systems is $\mathcal{O}(mn)$.  Task Bench's design reduces this work to $\mathcal{O}(m + n)$,
enabling dramatically more systems and benchmarks to be explored for the same amount of programming
effort.  New benchmarks created with Task Bench
immediately run on all systems, and new systems that implement the Task Bench interface immediately run all
benchmarks. 

Benchmarks in Task Bench are based on the observation that regardless
of the programming system in which an application is written, many
applications can be modeled as coarse-grain units of work, called
\emph{tasks}, with dependencies between tasks representing the
communication and synchronization required for parallel and
distributed execution. By explicitly modeling the \emph{task graph}
(with tasks as vertices and dependencies as edges), we make it
possible to explore a wide variety of patterns relevant to
parallel and distributed computing: trivial parallelism, halo exchanges (as
in structured and unstructured mesh codes), sweeps (as
in the discrete ordinates method of simulating radiation), FFTs, trees
(for divide and conquer), DNNs, graph analytics, etc. Tasks execute kernels with a
variety of computational properties, including compute- and
memory-bound loops of varying duration. Dependencies can be configured to carry communication payloads of varying size. Finally, multiple
(potentially heterogeneous) task graphs can be executed concurrently
to introduce task parallelism into the workload. Together, these
elements enable the exploration of a large space of application
behaviors---and make it easy to explore cases limited by runtime
overhead as well as ones where computation or communication is
dominant.

Adding a system to Task Bench involves implementing a set of standard
services, such as executing a task or data transfer. Though
benchmarks are described in terms of task graphs, this is simply a
convenient representation of the computation, and the underlying
system need not provide any native support for tasks. We provide
Task Bench implementations in systems as diverse as MPI and
Spark. Task Bench provides a core API that encapsulates functionality
shared among systems, which reduces implementation effort and makes it
much easier to achieve truly apples-to-apples comparisons between
systems.

This approach has allowed us to benchmark 15 very different parallel
and distributed programming systems (see
Table~\ref{tab:systems}).  By running all systems on common benchmarks
we were able to quantify phenomena
that have never before been measured.
Most strikingly, the overheads of systems we examine vary by more than five orders
of magnitude, with popular, widely used systems at both ends of the spectrum!  Clearly,
slower systems have ``good enough'' performance for some applications, while presumably
providing advantages in programmer productivity.

How does one predict whether performance will be good enough for a given application?
The most commonly reported metrics,
\emph{weak} and \emph{strong} scaling, do not directly characterize
the performance of the underlying
programming system. Weak scaling can hide arbitrary amounts of runtime
system overhead by using sufficiently large problem sizes, and strong
scaling does not separate runtime system overhead from application costs
(such as communication) that scale with the number of nodes when
using progressively larger portions of a machine. 

To characterize the contribution of runtime overheads
to application performance, and as an example of the novel studies that can be done
with Task Bench, we introduce a new metric called
\emph{minimum effective task granularity} (METG). Intuitively, for a given
workload, METG(50\%) is the smallest task granularity that maintains
at least 50\% efficiency, meaning that the application achieves at
least 50\% of the highest performance (in FLOP/s, B/s, or other
application-specific measure) achieved on a given
machine. The efficiency bound in METG is a key innovation over
previous approaches, such as \emph{tasks per second} (TPS), that fail
to consider the amount of useful work performed (if tasks are
non-empty~\cite{Canary16, Armstrong14}) or to perform useful work at all (if tasks are empty~\cite{LegionTracing18}).

METG captures the important essence of a
weak or strong scaling study, the behavior at the limit of
scalability. For weak scaling, METG(50\%) corresponds to the
smallest problem size that can be weak-scaled with 50\%
efficiency. For strong scaling, METG(50\%) can be used to compute the
scale at which efficiency can be expected to dip below 50\%.
We note that METG(50\%) for a given runtime system will
vary with the application and the underlying hardware---i.e., METG(50\%)
is not a constant for a given system, but we find that systems have
a characteristic range of METG(50\%) values and that there is additional insight
in the reasons that METG can vary.

A lower METG does not necessarily mean that
performance for a particular workload is better. Two systems with METG(50\%) of 100~\textmu{}s and 1~ms,
respectively, running an application with 10~ms average task granularity, are both likely to perform well. Only when task
granularity approaches (or drops below) METG(50\%) will they
likely diverge. METG identifies the regime in which a
given system can deliver good performance, and explains how
different systems coexist with runtime overheads that vary by orders of magnitude.

%
% FIXME:  Move these two paragraphs out of the introduction and into one of the METG sections.
%
\zap{
Task Bench and METG address issues common in limit studies of runtime
systems for parallel and distributed programming. Such studies often
employ the metric \emph{tasks per second} (TPS), which is almost
universally measured with trivial (i.e., no) dependencies \cite{LegionTracing18, Canary16, Armstrong14}. While
phrased in terms of tasks, TPS can be measured for any system as long
as the application in question has identifiable units of work that run
to completion without interruption. TPS is an upper bound on
runtime-limited application throughput. But it is not a tight bound, as the
cost of non-trivial dependencies can be significant. This issue can be easily fixed by running non-trivial
configurations of Task Bench.

There is another, deeper issue with TPS. TPS may be measured with
empty tasks~\cite{LegionTracing18} or with tasks of some
duration~\cite{Canary16, Armstrong14}. When using empty tasks, the
resulting upper bound on task scheduling throughput fails to represent
useful work within a realistic application. With non-empty tasks,
\emph{where the efficiency of the overall application is not
 reported}, TPS is not a measurement of runtime-limited
performance. Large tasks may be used to hide any amount of runtime
overhead, while small tasks may result in a drop in total
application throughput even as TPS increases. Only by constraining
efficiency, as in METG, can we meaningfully measure how runtime
overhead impacts the ability to perform useful application work.
} % zap

% Seems to be nearly impossible to get rid of the hyphen, so just give
% up and allow this paragraph to break across it.
\let\oldbrokenpenalty\brokenpenalty
%% \brokenpenalty=0

We conduct a comprehensive study of all 15 Task Bench implementations on
up to 256 Haswell nodes of the Cori supercomputer~\cite{Cori}.
%% :
%% Chapel \cite{Chapel15}, Charm++ \cite{Charmpp93}, Dask \cite{Dask15}, MPI \cite{MPI}, MPI+X (OpenMP, CUDA),
%% OmpSs \cite{OmpSs11}, OpenMP \cite{OpenMPSpec40},
%% PaRSEC \cite{PARSEC13}, Realm \cite{Realm14}, Regent \cite{Regent15},
%% Spark \cite{Spark10}, StarPU \cite{StarPU11},
%% Swift/T \cite{Wozniak13}, TensorFlow \cite{TensorFlow15}, and
%% X10 \cite{X1005}.
Using METG, we find that a number of factors---node
count, accelerators, and complex dependencies, among
others---individually or in combination contribute to an order of
magnitude or greater increase in METG, even in the most efficient
systems. While some systems can achieve sub-microsecond METG(50\%) in
best-case scenarios, we show that a more realistic
bound for running nearly any application at scale is 100~\textmu{}s with
current technologies. Our study includes several asynchronous systems
designed to provide benefits such as overlapped computation
and communication. While small-scale benchmarks of these systems
suffer from increased overhead, we find that the benefits of these systems become
tangible at scale (provided the runtime overhead doesn't increase
beyond about 100~\textmu{}s per task).

%% \brokenpenalty=\oldbrokenpenalty

% Performance issues:
% Chapel (communication performance, core pinning)
% Dask (asymptotic complexity of dask.delayed)
% PaRSEC (task pruning with non-trivial task graphs)
% Realm (DMA system overhead)
% TensorFlow (constant folding not parallel)

Beyond comparative study, the ability to explore a large configuration
space also enables the discovery of bugs in the underlying systems. We
found five performance issues, ranging from communication efficiency
(Chapel, Realm), to the efficiency of task pruning, analysis and
constant folding (PaRSEC, Dask and TensorFlow). Three have been fixed
and all have been acknowledged by the developers of the respective
systems. (All were either fixed or worked around in our experiments.)
In some cases these correspond to order of magnitude (or even
asymptotic) improvements in the performance of the underlying
systems---benefits which apply well beyond Task Bench to all classes
of applications. The bugs are described in more detail in Section~\ref{sec:case-study}.

The paper is organized as follows: Section~\ref{sec:task-bench}
describes the Task Bench design. Section~\ref{sec:implementation}
discusses implementations in 15 systems.  Section~\ref{sec:metg}
defines METG and its relationship to quantities of interest to
application developers.  Section~\ref{sec:evaluation} provides a
comprehensive evaluation on Cori. Section~\ref{sec:case-study} describes bugs found with Task Bench. Section~\ref{sec:related-work} relates to
previous efforts; Section~\ref{sec:conclusion} concludes.
