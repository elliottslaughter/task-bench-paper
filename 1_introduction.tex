\section{Introduction}
\label{sec:introduction}

The challenge of parallel and distributed computation has led to a
wide variety of proposals for programming models, languages, and
runtime systems. But while these systems are well-represented in the literature, comprehensive and comparative performance evaluations
remain difficult to find. Our
goal in this paper is to develop a meaningful and useful framework for
comparing the performance of parallel and distributed programming
systems.


% note: removed definition of task

One approach to comparing the performance of different systems is through \emph{proxy-} or
\emph{mini-apps}. Because they distill the structure of a larger
application, mini-apps can provide insight
without the expense of developing a production code. However, despite the name, our experience is that
mini-apps still requite significant investment to develop
to the level of quality needed for useful benchmarking. In many cases,
the effort to tune for performance exceeds the effort to develop the
mini-app in the first place. As a result, implementations of mini-apps
often do not reach the level of maturity required to compare
systems. Indeed, there are few published studies
that involve more than a handful of systems~\cite{LULESH13}.

There is a second important issue: How does one predict how
the performance of a mini-app generalizes to a full
application, even one closely related? Part of the problem is
that \emph{weak} and \emph{strong} scaling, which are by far the most commonly
reported performance metrics,
do not directly characterize the performance of the underlying
programming system. Weak scaling can hide arbitrary amounts of runtime
system overhead with sufficiently large problem sizes, and strong
scaling does not separate runtime system overhead from other costs
(such as increased communication, among many other things) when
using progressively larger portions of a machine.

To address the first issue, we present Task Bench, a
\emph{parameterized} benchmark designed for exploring the performance
behaviors of parallel and distributed programming systems under a
variety of conditions. A critical property of Task Bench is that,
while able to express a wide variety of typical parallel programming
patterns, it is simple enough to be implemented with reasonable effort
to a level of quality sufficient for comparative studies.

A key insight of Task Bench is that many applications can be modeled
as a set of \emph{tasks}, or coarse-grain units of work, with
dependencies between tasks representing the communication required for
distributed execution. Task Bench consists of a configurable \emph{task graph} with tasks for
each point in an iteration space. Tasks are connected by dependencies
as determined by a \emph{dependence relation}. This design permits a
very concise description of a wide variety of patterns relevant to
scientific computing and large scale data analysis: trivial parallelism, halo exchanges (such as
seen in structured and unstructured mesh codes), sweeps (such as used
in the discrete ordinates method of radiation simulation), FFTs, trees
(for divide and conquer algorithms), and so on. The tasks at each
point in the task graph can be configured to execute kernels with a
variety of computational properties, including compute- and
memory-bound operations of varying durations. The dependencies between
tasks can be configured to carry communication payloads of varying size, permitting
the design of communication-bound cases. Finally, multiple
(potentially heterogeneous) task graphs can be executed concurrently
to introduce task parallelism into the workload. Together, these
design elements permit the exploration of a large space of application
behaviors.

To address the second issue and isolate the contribution of the programming model's runtime system
to application performance, we introduce a new metric called
\emph{minimum effective task granularity} (METG). Intuitively,
METG(50\%) is the smallest application task granularity which can be
executed while maintaining at least 50\% efficiency,
meaning that the application achieves at least 50\% of peak
performance (e.g., FLOPS or memory bandwidth) 
on a given machine. METG captures the essence of what is important in a weak
or strong scaling study, the behavior at the limit of scalability. For
weak scaling, METG(50\%) corresponds directly to the smallest problem
size that can be weak scaled with 50\% efficiency. For strong scaling,
METG(50\%) can be used to compute the scale at which performance can
be expected to dip below 50\% efficiency.

A lower METG does not necessarily mean that performance for a particular workload will be
better. For example, two systems with METG(50\%) of 100~us and 1~ms,
respectively, running an application with an average task granularity
of 10~ms, are both very likely to perform well. Only when the task
granularity approaches (or drops below) METG(50\%) will performance
likely diverge. Thus METG helps to identify the regime in which a
given system can deliver good performance.

Task Bench and METG address issues common in limit studies of runtime
systems for parallel and distributed programming. Such studies often
employ the metric \emph{tasks per second} (TPS), which is almost
universally measured with trivial (i.e., no) dependencies \cite{LegionTracing18, Canary16, Armstrong14}. While
phrased in terms of tasks, TPS can be measured for any system as long
as the application in question has identifiable units of work that run
to completion without interruption. TPS is an upper bound on
runtime-limited application throughput, but not a tight bound, as the
cost of non-trivial dependencies can be significant and may impact
scalability. This issue is easily fixed by running non-trivial
configurations of Task Bench.

There is another, deeper issue with TPS. TPS may be measured with
empty tasks~\cite{LegionTracing18} or with tasks of some
duration~\cite{Canary16, Armstrong14}. If empty tasks are used, the
resulting upper bound on task scheduling throughput fails to represent
any realistic application, as no
useful work is being performed. If non-empty tasks are used, \emph{but
  the efficiency of the overall application is not reported}, then TPS
is not a measurement of runtime-limited performance. Large tasks may
be used to hide any amount of runtime overhead,
while tasks that are too small may result in a drop in total
application throughput even as TPS increases. Only by constraining
efficiency, as in METG, can we meaningfully measure how runtime
overhead impacts the ability perform useful application work.

We conduct a comprehensive comparative study of Task Bench
implementations in 13 different programming systems, including
Chapel \cite{Chapel15}, Charm++ \cite{Charmpp93}, MPI \cite{MPI},
OmpSs \cite{OmpSs11}, OpenMP \cite{OpenMPSpec40},
PaRSEC \cite{PARSEC13}, Realm \cite{Realm14}, Regent \cite{Regent15},
Spark \cite{Spark10}, StarPU \cite{StarPU11},
Swift/T \cite{Wozniak13}, TensorFlow \cite{TensorFlow15}, and
X10 \cite{X1005}. Using METG(50\%), we determine the baseline
runtime overhead of each system along with overheads associated with
increasing numbers of and/or variations in the pattern of dependencies. We
also explore each system's ability to scale, hide communication
overhead, and mitigate load imbalance.

The paper is organized as follows:

\begin{itemize}
\item Section~\ref{sec:metg} defines the METG(50\%) metric and its
  relationship to quantities of interest to application developers.
\item Section~\ref{sec:task-bench} describes the design of Task Bench.
\item Section~\ref{sec:implementation} details the implementation of
  Task Bench in 13 programming systems.
\item Section~\ref{sec:evaluation} provides a comprehensive evaluation
  of Task Bench on up to 256 Haswell nodes of the Cori
  supercomputer~\cite{Cori}.
\end{itemize}

Section~\ref{sec:related-work} relates this work to previous efforts,
and Section~\ref{sec:conclusion} concludes.
