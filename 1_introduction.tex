\section{Introduction}
\label{sec:introduction}

The challenge of parallel and distributed computation has led to a
wide variety of proposals for programming models, languages, and
runtime systems. While these systems are well-represented in the literature, comprehensive and comparative performance evaluations
remain difficult to find. Our
goal in this paper is to develop a meaningful and useful framework for
comparing the performance of parallel and distributed programming
systems, to help users and developers evaluate the performance tradeoffs of these systems.

One approach to comparing the performance of different systems is through \emph{proxy-} or
\emph{mini-apps}. Because they distill the key computational characteristics of a larger
application, mini-apps can provide insight
without the expense of developing a production code. However, despite the name, our experience is that
mini-apps still require significant investment to develop
to the level of quality needed for useful benchmarking. In many cases,
the effort to tune for performance exceeds the effort to develop a correct implementation. As a result, implementations of mini-apps
often do not reach the level of maturity required to compare
systems. Few published studies compare more than a handful of systems~\cite{LULESH13}.

We present Task Bench, a parameterized benchmark for exploring the performance
of parallel and distributed programming systems under a
variety of conditions.  The key property of Task Bench is that it completely separates
the system-specific implementation from the implementation
of the benchmarks themselves.
In all previous benchmarks we know of, the effort to implement $m$ benchmarks on $n$
systems is $\mathcal{O}(mn)$.  Task Bench's design reduces this work to $\mathcal{O}(m + n)$,
enabling dramatically more systems and benchmarks to be explored for the same amount of programming
effort.  New benchmarks created with Task Bench
immediately run on all systems, and new systems that implement the Task Bench interface immediately run all
benchmarks. 

Benchmarks in Task Bench are based on the observation that many applications can be modeled
as a set of \emph{tasks}, or coarse-grain units of work, with
dependencies between tasks representing the communication and synchronization required for
distributed execution. A benchmark consists of a \emph{task graph} with tasks for
each point in an \emph{iteration space}, and dependencies
determined by a \emph{dependence relation}. This design permits a
concise description of a wide variety of patterns relevant to
scientific computing and large scale data analysis: trivial parallelism, halo exchanges (such as
seen in structured and unstructured mesh codes), sweeps (such as used
in the discrete ordinates method of radiation simulation), FFTs, trees
(for divide and conquer algorithms), and so on. Tasks execute kernels with a
variety of computational properties, including compute- and
memory-bound operations of varying durations. Dependencies between
tasks can be configured to carry communication payloads of varying size, permitting
the design of communication-bound cases. Finally, multiple
(potentially heterogeneous) task graphs can be executed concurrently
to introduce task parallelism into the workload. Together, these
design elements enable the exploration of a large space of application
behaviors.

Adding a new programming system to Task Bench involves  implementing a set of standard
services, such as executing a task or a data transfer.  This interface is simple enough to 
be implemented with reasonable effort to a level of quality sufficient for comparative
studies. Task Bench provides a core library that encapsulates the functionality
shared among systems, which not only reduces the implementation effort
required but also makes it much easier to achieve truly apples-to-apples comparisons
between systems.

Using Task Bench we were able to quickly incorporate 15 very different parallel and distributed
runtime systems.  By running all systems on common benchmarks we were able to quantify phenomena
that have never before been measured.
Most strikingly, the performance of the systems we examine varies by more than five orders
of magnitude, with very popular, widely used systems at both ends of the spectrum!  Clearly
the slower systems have ``good enough'' performance for some class of applications, while presumably
providing advantages in programmer productivity.

This result points to a natural question:   How does one predict whether the performance of a runtime
system will in fact be good enough for a particular application?  In trying to answer this question,
we realized that \emph{weak} and \emph{strong} scaling, by far the most commonly
reported performance metrics, do not directly characterize the performance of the underlying
programming system. Weak scaling can hide arbitrary amounts of runtime
system overhead by using sufficiently large problem sizes, and strong
scaling does not separate runtime system overhead from costs
(such as increased communication) that scale with the number of nodes when
using progressively larger portions of a machine. 

To characterize the contribution of runtime overheads
to application performance, and as an example of the novel studies that can be done
with Task Bench, we introduce a new metric called
\emph{minimum effective task granularity} (METG). Intuitively, for a given
workload, METG(50\%) is the smallest task granularity that maintains
at least 50\% efficiency, meaning that the application achieves at
least 50\% of the higest performance (in FLOP/s, B/s, or other
application-specific measure) achieved on a given
machine. The efficiency bound in METG is a key innovation over
previous approaches, such as \emph{tasks per second} (TPS), that fail
to consider the amount of useful work peformed (if tasks are
non-empty~\cite{Canary16, Armstrong14}) or to perform useful work at all (if tasks are empty~\cite{LegionTracing18}).

METG captures the essence of what is important in a
weak or strong scaling study, the behavior at the limit of
scalability. For weak scaling, METG(50\%) corresponds to the
smallest problem size that can be weak-scaled with 50\%
efficiency. For strong scaling, METG(50\%) can be used to compute the
scale at which performance can be expected to dip below 50\%
efficiency.  We note that METG(50\%) for a given runtime system will
vary with the particular application and the underlying hardware---i.e., METG(50\%)
is not a constant for a given system, but we will see that individual systems have
a characteristic range of METG(50\%) values and that there is additional insight
in the reasons that METG can vary.

A lower METG does not necessarily mean that
performance for a particular workload is better. For example, two systems with METG(50\%) of 100~\textmu{}s and 1~ms,
respectively, running an application with an average task granularity
of 10~ms, are both very likely to perform well. Only when the task
granularity approaches (or drops below) METG(50\%) will performance
likely diverge. Thus METG helps to identify the regime in which a
given system can deliver good performance, and explains how
different systems coexist with runtime overheads that vary by orders of magnitude.

%
% FIXME:  Move these two paragraphs out of the introduction and into one of the METG sections.
%
\zap{
Task Bench and METG address issues common in limit studies of runtime
systems for parallel and distributed programming. Such studies often
employ the metric \emph{tasks per second} (TPS), which is almost
universally measured with trivial (i.e., no) dependencies \cite{LegionTracing18, Canary16, Armstrong14}. While
phrased in terms of tasks, TPS can be measured for any system as long
as the application in question has identifiable units of work that run
to completion without interruption. TPS is an upper bound on
runtime-limited application throughput. But it is not a tight bound, as the
cost of non-trivial dependencies can be significant. This issue can be easily fixed by running non-trivial
configurations of Task Bench.

There is another, deeper issue with TPS. TPS may be measured with
empty tasks~\cite{LegionTracing18} or with tasks of some
duration~\cite{Canary16, Armstrong14}. When using empty tasks, the
resulting upper bound on task scheduling throughput fails to represent
useful work within a realistic application. With non-empty tasks,
\emph{where the efficiency of the overall application is not
 reported}, TPS is not a measurement of runtime-limited
performance. Large tasks may be used to hide any amount of runtime
overhead, while small tasks may result in a drop in total
application throughput even as TPS increases. Only by constraining
efficiency, as in METG, can we meaningfully measure how runtime
overhead impacts the ability to perform useful application work.
} % zap

We conduct a comprehensive comparative study of Task Bench
implementations in 15 programming systems:
Chapel \cite{Chapel15}, Charm++ \cite{Charmpp93}, Dask \cite{Dask15}, MPI \cite{MPI}, MPI+X (OpenMP and CUDA),
OmpSs \cite{OmpSs11}, OpenMP \cite{OpenMPSpec40},
PaRSEC \cite{PARSEC13}, Realm \cite{Realm14}, Regent \cite{Regent15},
Spark \cite{Spark10}, StarPU \cite{StarPU11},
Swift/T \cite{Wozniak13}, TensorFlow \cite{TensorFlow15}, and
X10 \cite{X1005}. Using METG, we find a number of factors---node
count, accelerators, and complex dependencies, among
others---individually or in combination contribute to an order of
magnitude or greater increase in METG, even in the most efficient
systems. While in the most simplified scenarios, some systems can
achieve a METG(50\%) of 390~ns, we show that a more realistic
bound for running nearly any application at scale is 100~\textmu{}s with
current technologies.

%% This implies that simplifying the application space too much
%% (e.g., by using trivial dependencies) risks overemphasizing the
%% importance of the lightweight mechanisms in such systems---a risk that
%% can be mitigated by using Task Bench's more complete coverage of the
%% space of application scenarios.

\zap{
Using METG, we determine the baseline
runtime overhead of each system along with overheads associated with
increasing numbers of and/or variations in the pattern of dependencies. We
also explore each system's ability to scale, hide communication
overhead, and mitigate load imbalance.
} % zap

The paper is organized as follows: Section~\ref{sec:task-bench}
describes the design of Task Bench, Section~\ref{sec:implementation}
discusses the implementation of Task Bench in 15 programming systems,
Section~\ref{sec:metg} defines the METG metric and its
relationship to quantities of interest to application developers,
Section~\ref{sec:evaluation} provides a comprehensive evaluation of
Task Bench on up to 256 Haswell nodes of the Cori
supercomputer~\cite{Cori}, Section~\ref{sec:related-work} relates this
work to previous efforts, and Section~\ref{sec:conclusion} concludes.
