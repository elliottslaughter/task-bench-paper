\section{Evaluation}
\label{sec:evaluation}

We present a comprehensive evaluation of our Task Bench implementations on up to 256
Haswell nodes of the Cori supercomputer~\cite{Cori}, a Cray XC40
machine. Cori Haswell nodes have 2 sockets with Intel Xeon E5-2698 v3
processors (a total of 32 physical cores per node), 128 GB RAM, and a
Cray Aries interconnect. We use GCC 7.3.0 for all Task Bench
implementations, and (where applicable) the system default MPI
implementation, Cray MPICH 7.7.3. Versions and flags for the
various systems are shown in Table~\ref{tab:flags}.

\input{e1_flops}
\input{e2_efficiency}
\input{e7_bytes}

\input{e3_metg_compute}

For GPU experiments we use Piz Daint~\cite{PizDaint}, a Cray XC50 with
a Intel Xeon E5-2690 v3 (12 physical cores) and one NVIDIA Tesla
P100 per node. We use GCC 6.2.0, Cray MPICH 7.7.2, and
CUDA 9.1.85.

\subsection{Compute Kernel Performance}
\label{subsec:peak-performance-and-efficiency}

We first consider the peak performance achieved by each system. There
should exist some task granularity which is sufficient to offset the
runtime overheads of any system, regardless of how large those
are. Even so, a variety of issues can lead to performance loss
(e.g. due to not using all available cores). Verifying that peak
performance is achieved ensures that there are no such flaws in our
configuration.

Figure~\ref{fig:flops}, which is the full version of
Figure~\ref{fig:flops-mpi}, shows the FLOP/s achieved with a compute-bound
kernel with varying problem sizes (simulated by running the kernel for varying numbers of iterations). Each data point in the graph is a mean of 5 runs, with Task Bench configured to execute 1000 time steps of the stencil pattern. In the best case, we measure peak FLOP/s of
$1.26 \times 10^{12}$, which compares favorably with the officially
reported number of $1.2 \times 10^{12}$ \cite{Cori}. We use our empirically determined number as
the baseline for 100\% efficiency below.

Most systems achieve or nearly achieve peak FLOP/s. Some
systems reserve a number of cores (usually 1 or 2) for internal
use (see below); these systems take a minor hit in peak FLOP/s compared
to systems that share all cores between application and runtime. Some
higher-overhead systems struggle to achieve peak FLOP/s, though in most cases
the curves suggest that performance would continue to improve if we
were to run larger problem sizes. Unfortunately, the excessive
computational cost of running such tests makes this prohibitively
expensive. For example, the Spark job in
this case ran for over 6 hours.

Figure~\ref{fig:efficiency} plots efficiency (as a percentage of
peak FLOP/s) vs. task granularity. As described in Section~\ref{sec:metg}, this is
used to calculate METG(50\%). The
red, dashed line indicates 50\% efficiency.
In most cases, task granularity asymptotes prior to this point,
though some systems continue to improve at lower values. Accounting for this effect is one of the main arguments in
favor of using reasonable efficiency thresholds for METG instead of
empty tasks
(i.e., METG(0\%)). Empty tasks
reward strategies, such as devoting 100\% of
system resources to the runtime system, that make no sense for real
applications.

\subsection{Memory Kernel Performance}

Figure~\ref{fig:bytes} shows performance with a memory-bound kernel. We measure a peak memory
bandwidth of 79~GB/s, using a working set size of 0.5 GB. As discussed in Section~\ref{sec:task-bench}, the kernels are designed to keep the working set constant as the number of iterations decrease to avoid noisy, superlinear effects in the results. For comparison, the
OpenMP-enabled STREAM benchmarks~\cite{STREAM} report up to 98~GB/s on the same hardware.

Not all cores are required to saturate memory bandwidth.
  This reduces the impact of reserving cores for system use
  (e.g. task-based systems that perform dependence analysis). Nearly
all systems hit 100\% of peak, unlike the compute-bound case.

The remaining experiments use compute-bound kernels.

% FIXME: why compute-bound kernels?

\subsection{Baseline Overhead}

One question when considering different programming
systems is: How much overhead does the system add? This question is tricky to answer directly because some systems introduce
overhead \emph{inline} (i.e., by running system internal processes on
the same cores as application tasks), while other systems introduce
overhead \emph{out-of-line} (i.e., by dedicating one or more cores
solely to runtime use). Some systems, like Charm++, PaRSEC, Realm, and Regent,
support both configurations.

To answer this question, we use METG %% to determine the
%% smallest task granularity that can be executed at a given level of
%% efficiency
as a proxy for overhead. Figure~\ref{fig:metg-compute}
shows how METG(50\%) varies with node count for a subset of
dependence patterns supported by Task Bench. METG(50\%) is calculated
separately at each node count, to distinguish runtime system behavior from
changes in communication latency and topology when using
progressively larger portions of the machine.

We consider the following configurations of Task Bench:
Figure~\ref{fig:metg-compute-stencil} is a 1D stencil where each task
depends on 3 other tasks (including the same point in the previous
timestep). Figure~\ref{fig:metg-compute-nearest} is a pattern where
each task depends on 5 others, chosen to be as close as
possible. Figure~\ref{fig:metg-compute-spread} is a pattern where each
task depends on 5 others, spread as widely as possible. And
Figure~\ref{fig:metg-compute-4x-nearest} shows 4 identical copies of
the nearest dependence pattern executing concurrently.

We observe that overheads vary by over 5 orders of
magnitude. The most efficient systems are explicitly parallel
and provide very lightweight mechanisms for parallelism. Task-based systems for HPC tend to be next most efficient, and
provide additional features such as automatic dependency discovery and
data movement. Higher overhead systems tend to be designed primarily for
large-scale data analysis or workflows. It is worth
remembering that these are \emph{minimum}
effective task granularities. Applications with an average
task granularity of \emph{at least} this value can usually be expected
to execute efficiently. Typical task granularities will
generally be determined by the application domain being
considered. Most notably, for large-scale data analytics workloads, the higher METG values observed for Spark are sufficient. In contrast, for high-performance
scientific simulations, task granularities in the millisecond range
are useful, as such applications communicate (e.g., for halo
exchanges) much more frequently.

The least complicated pattern (stencil) is most favorable
to MPI, as it provides no
opportunity for task parallelism. The
dominating factor in this case is the overhead of executing a task, which is
minimal for MPI as the code simply executes tasks in alternation with
communication. The asynchrony
of other systems is pure overhead in this scenario.
MPI's advantage shrinks as complexity grows, and even reverses as task parallelism is
added in the form of multiple task graphs.

We omit Spark and Swift/T
with more complicated dependencies, as their higher overheads require
excessive problem sizes (beyond what completes in 6
hours) to reach 50\% efficiency.

\subsection{Scalability}
\label{subsec:scalability}

METG summarizes system overheads in a single number. This makes it possible to
evaluate how communication topology and latency impact METG at
different node counts, as shown in Figure~\ref{fig:metg-compute}.
We find that
systems with the smallest METG on one node have roughly an order
of magnitude higher METG at 256 nodes. Increased communication latencies require significantly larger tasks to
achieve the same level of efficiency, so apparent differences in
overhead at small node counts can matter much less or not at
all at larger node counts.

Most systems for HPC are highly scalable,
but this is not true of all the systems included in this
evaluation. Lower is better in Figure~\ref{fig:metg-compute}, and
flat is ideal. Lines that rise with node count indicate less than
ideal scaling. Most notably, Spark is primarily
intended for industrial data center applications with task
granularities measured in seconds. Spark uses a centralized
controller, which limits throughput, and this is visible in the figure
as the line for Spark immediately rises with node count. Keep in mind
that Spark is being evaluated here with a nontrivial dependence
pattern that is relatively unrepresentative of Spark's normal use
cases. Spark is more efficient with trivial parallelism, as described
in Section~\ref{subsec:number-of-dependencies}.

\input{e4_radix}

\input{e6_communication}

PaRSEC, StarPU and Regent rely on runtime analysis that can suffer
from scalability bottlenecks if every node must consider the tasks
executing on all other nodes. Although all three systems offer ways to
improve scalability, these methods are not equally effective. PaRSEC
DTD and StarPU allow users to manually prune tasks to reduce overhead;
however the checks are not reduced to zero. Similarly, PaRSEC PTG uses
compile-time optimizations to avoid the need for manual pruning, but
still targets the PaRSEC DTD runtime and thus incurs some
overhead. Figure~\ref{fig:metg-compute} shows that these models do not
provide ideal scalability, as seen by METG values that rise with
increasing node count. Regent uses a compile-time optimization to
generate code with a constant overhead per
node~\cite{ControlReplication17}. Of the three systems, Regent is the
only one that achieves ideal scalability while preserving its
original, implicitly parallel programming model. The others can
achieve ideal scalability but require increasing levels of manual
intervention. PaRSEC \lstinline{shard} includes additional manual
optimizations over DTD to completely eliminate dynamic checks. StarPU
\lstinline{expl} is written in an explicitly parallel style using MPI
for communication, and thus avoids any analysis bottleneck. These
results indicate that the underlying systems are capable of scalable
execution, but that the dynamic checks incurred by the implicitly
parallel programming models hinder that scalability.

\subsection{Number of Dependencies}
\label{subsec:number-of-dependencies}

The number of dependencies per task has a strong influence on
overhead, as shown in
Figure~\ref{fig:radix}. This plot shows METG(50\%) for the nearest
dependence pattern, when varying the number of dependencies per task
from 0 to 9.

% FIXME: make sure these numbers are up to date for final paper

The ratio in METG between 0 and 3 dependencies per task ranges from
$0.82\times$ to $250\times$ (mean $21\times$, std. dev. $64$). The
large standard deviation shows that the sensitivity of system overhead
to the dependency pattern varies widely. The largest ratios are among
systems that perform runtime work inline. For example, MPI achieves an
METG of 390 ns with 0 dependencies, but this rises to 4.6 \textmu{}s
with 3 dependencies, a $12\times$ increase. This is unsurprising, as
with 0 dependencies no \lstinline[language=C++]{MPI_Isend} calls are
issued at all. Clearly, choosing a representative dependence pattern
is important when estimating the performance of a workload or class of
workloads.

\subsection{Overlapping Communication and Computation}

Also of interest is the ability to
hide communication latency in the presence of task
parallelism. Figure~\ref{fig:efficiency-communication} plots efficiency with varying amounts of
communication, determined by the number
of bytes produced by each task (and therefore communicated with each
task dependency).

Asynchronous systems such as Charm++ demonstrate two benefits in
these plots. First, by overlapping communication with computation,
such systems execute smaller task granularities at higher
levels of efficiency compared to the MPI
implementations. Second, the asynchrony and scheduling flexibility from
executing multiple graphs also makes the curves smoother,
as spikes in latency due to interference from other jobs can be
mitigated, leading to more predictable performance, especially at
smaller message sizes.

The effectiveness of such overlap can be influenced by the scheduling
policies of the underlying system. For example, Chapel's default
scheduler uses a round-robin policy; we see in
Figure~\ref{fig:efficiency-communication} that this approach fails to
take full advantage of the available task parallelism. A work-stealing
scheduler (Chapel \lstinline{distrib}) is able to recover this
performance.

\subsection{Load Imbalance}

\input{e5_imbalance}

One advantage of asynchronous execution is
the ability to mitigate load imbalance with little or no additional programmer effort, especially in the presence of
task parallelism. To quantify this effect,
Figure~\ref{fig:efficiency-imbalance} plots task granularity vs.
efficiency under load imbalance where each task's duration is multiplied by a uniform random variable in [0,~1). Task durations are generated with a deterministic
pseudo random number generator with a consistent seed to ensure
identical durations for all systems.

The MPI Task Bench, with its distinct computation and communication phases,
suffers the most under load imbalance. The biggest
difference is at large task granularities, where the imbalance
effectively puts an upper bound on efficiency. At smaller task
granularities the effect shrinks and may even reverse as systems hit
their fundamental limits due to overhead.

The remaining differences are due primarily to
different scheduling behaviors. The execution of 4
simultaneous task graphs only partially mitigates the
load imbalance between tasks. Systems that provide an
additional on-node work stealing capability (such as Chapel with the \texttt{distrib} scheduler) see additional gains in
efficiency at large task granularities. However, the use of
work-stealing queues can also impact throughput at small task
granularities. For example, Chapel's default (non-work-stealing) scheduler outperforms \texttt{distrib} at very small task granularities. We do not consider Charm++ load balancers because the imbalance is \emph{non-persistent} (i.e., timestep $t$ is uncorrelated with timestep $t+1$). We leave analysis of persistent load imbalance to future work.

\subsection{Heterogeneous Processors}

\input{e10_cuda_efficiency}

To determine the cost of scheduling tasks on GPUs, Figure~\ref{fig:cuda-efficiency} compares MPI
and MPI+CUDA on Piz Daint~\cite{PizDaint}. The CUDA
compute kernel achieves $4.759 \times 10^{12}$
FLOP/s, which is very close to the officially reported number
$4.761 \times 10^{12}$. The CPU achieves $5.726 \times 10^{11}$
FLOP/s. Note that the kernels perform different numbers of
operations as the GPU requires more work to reach peak
performance. The x-axis in Figure~\ref{fig:cuda-efficiency} is
normalized to keep FLOPs constant for a given problem
size.

Our MPI+CUDA code uses an offload model with data copied to/from the GPU every step. In our tests, \lstinline{w1} uses 1 task per GPU, whereas
\lstinline{w4} overdecomposes, using 4 MPI
ranks per GPU to push work to the GPU in parallel.
\lstinline{w4} achieves higher FLOP/s but
drops more rapidly at small problem sizes, due to the overhead of
running $4\times$ as many CUDA kernels. Either way,
GPUs require more work to achieve high performance, and the overhead
of copying data dominates at small task
granularities, where CPUs achieve higher
performance. While Figure~\ref{fig:cuda-efficiency} is not couched in
terms of METG (as peak performance on CPU and GPU are very
different), the conclusion here is similar to
Section~\ref{subsec:scalability}: the cost of sending data and
tasks to GPUs imposes a floor on task granularity relative to CPUs, reducing the advantage at small task granularities of
very lightweight mechanisms such as those in MPI.

\subsection{Validating METG with Time to Solution}

% Summary:

% Charm++:
% Limit/Ideal intersects at: 55 nodes, 0.068 s
% Limit/Actual intersects at: 67 nodes, 0.086 s
% Limit/Ideal vs Limit/Actual gap in nodes: 1.22x
% Limit/Ideal vs Limit/Actual gap in time: 1.27x

We can use METG to predict the scalability of a code. Figure
\ref{fig:strong-scaling} shows strong scaling for three Task Bench
implementations with the stencil pattern. Lines marked ``actual''
represent strong scaling measurements, while ones marked ``limit 50\%'' are
computed by multiplying METG(50\%) by tasks per core (in this case,
1000) to obtain wall clock time. Intuitively, ``limit'' is the smallest time to solution that
can be achieved for \emph{any} problem size at that node count, while
maintaining 50\% efficiency. Data points where ``actual'' falls below
``limit'' mark points where strong scaling parallel efficiency is less
than 50\%.

There are two points of interest on each ``limit 50\%'' curve: the point
where it intersects ``actual'' and the point where it intersects an
ideal scaling curve, computed by taking the initial time to solution
and assuming linear scaling. These points are marked for Charm++ in
Figure~\ref{fig:strong-scaling} with a black square and black circle,
respectively.
Notably, the ideal-limit intersection (black circle) requires only a
run of the application on one node, combined with METG(50\%)
measurements, to estimate the strong scalability of
the code:
%% Notably, computing the ideal scaling line requires only
%% the running time on a single node, so the intersection between ideal
%% and ``limit'' can be found with only the single-node time to
%% solution, along with measurements of METG(50\%) at each node
%% count. Using this data it is possible to predict the
%% strong scalability of the code:
i.e., the smallest time to solution,
and the number of nodes at which that time is achieved, while
maintaining at least 50\% efficiency. In
Figure~\ref{fig:strong-scaling}, the error in the estimate is the distance
between the black square and circle; these are separated by a factor of
$1.22\times$ in node count, and $1.27\times$ in time to solution.

\input{e9_strong}
\input{e11_metg_predict_strong}

Table~\ref{tab:metg-predict-strong} expands this comparison to the 4
patterns tested in Section~\ref{subsec:scalability} for all 11
programming systems that scale well enough to evaluate the separation
between the limit-ideal and limit-actual intersections. We see that
overall, the mean separation is at most $1.96\times$ in node count
and at most $1.29\times$ in time to solution, making this a
useful way to predict strong scaling in the 4 patterns we tested in
Section~\ref{subsec:scalability}.

\zap{
When we consider all 11 programming systems that scale well enough to
perform this experiment, we find that the average separation between
the ideal and actual intersection points is $1.61\times$ in node count
(max $2.19\times$, min $1.13\times$) and $1.28\times$ in time to
solution (max $1.69\times$, min $1.01\times$).
}

\zap{
\input{e8_weak}
\input{e9_strong}
\subsection{Comparison with Weak and Strong Scaling}

Figures~\ref{fig:weak-scaling} and \ref{fig:strong-scaling} show
conventional strong and weak scaling of the stencil pattern for
comparison with the METG results in
Figure~\ref{fig:metg-compute-stencil}. At small node counts, where the
average task granularity is larger than the METG for most systems, the
performance is ideal. For several systems METG rises with node count,
and this becomes visible in the weak and strong scaling graphs as
non-ideal performance. Thus the shape of the weak and strong scaling
graphs effectively interpolates between ideal performance and the METG
curve depending on how close each system is to being limited by
overhead.
} % zap
