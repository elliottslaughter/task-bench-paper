\section{Evaluation}
\label{sec:evaluation}

\input{e0_flags}

We present a comprehensive evaluation of our Task Bench implementations on up to 256
Haswell nodes of the Cori supercomputer~\cite{Cori}, a Cray XC40
machine. Cori Haswell nodes have 2 sockets with Intel Xeon E5-2698 v3
processors (a total of 32 physical cores per node), 128 GB RAM, and a
Cray Aries interconnect. We use GCC 7.3.0 for all Task Bench
implementations, and (where applicable) the system default MPI
implementation, Cray MPICH 7.7.3. Versions and flags for the
various systems are shown in Table~\ref{tab:flags}.

\subsection{Compute-Bound Performance}
\label{subsec:peak-performance-and-efficiency}

In theory, any system should be able to hit peak performance as long as
the kernels are well-written and of sufficiently large granularity. In
practice, achieving peak performance is tricky, and many subtle pitfalls of implementation or configuration can easily lead to poor performance. Unfortunately, peak performance is also frequently skipped in evaluations of
runtime system overhead. However, verifying that peak performance is
achieved is important to ensuring that evaluations of overhead and
efficiency are well-grounded.

Figure~\ref{fig:flops} shows the FLOPS achieved with a compute-bound
kernel with varying problem sizes. This is the full version of
Figure~\ref{fig:flops-mpi}. Each data point in the graph is the mean of 5 runs, with Task Bench configured to execute 1000 time steps of the stencil pattern. In the best case, we measure peak FLOPS of
$1.26 \times 10^{12}$, which compares favorably with the officially
reported number of $1.2 \times 10^{12}$ \cite{Cori}. For the purposes
of measuring efficiency, we use our empirically determined number as
the baseline for 100\% efficiency.

\input{e1_flops}
\input{e2_efficiency}

The vast majority of systems achieve or nearly achieve peak FLOPS. Some
systems reserve a number of cores (usually 1 or 2) for internal
runtime usage; these systems take a minor hit in peak FLOPS compared
to systems which share all cores between the application and runtime. Some of the
slower systems struggle to achieve peak FLOPS, though in most cases
the curves suggest that performance would continue to improve if we
were to run larger problem sizes. Unfortunately, the excessive
computational cost of running such tests makes this prohibitively
expensive. For example, the Spark job in
this configuration ran for over 6 hours.

Figure~\ref{fig:efficiency} shows efficiency (as a percentage of
peak FLOPS) plotted against task granularity. This plot is
used to calculate METG(50\%), as described in Section~\ref{sec:metg}. The
red, dashed line shows the point where 50\% efficiency is achieved.
In most cases, task granularity asymptotes prior to the 50\% efficiency point,
though some systems continue to improve as efficiency continues
to drop. Accounting for this effect is one of the main arguments in
favor of METG with a reasonable efficiency threshold instead of
measuring task scheduling throughput of empty tasks
(effectively METG(0\%)). Measuring performance using empty tasks can
reward implementation strategies, such as devoting nearly 100\% of
system resources to the runtime system, that make no sense for real
applications.

\subsection{Memory-Bound Performance}

Figure~\ref{fig:bytes} shows the performance of Task Bench
implementations with a memory-bound kernel. We measure peak memory
bandwidth of 108~GB/s, which compares favorably with the
OpenMP-enabled STREAM benchmarks~\cite{STREAM} which report up to 98~GB/s.

Not all cores are required to saturate memory bandwidth. As a result,
systems that reserve one or more cores are able to hit 100\% of peak
performance, unlike with compute-bound kernels. Otherwise the
performance curves are similar. We are currently investigating the
performance of Charm++.

% FIXME: why is Charm++ slow?

The remaining experiments use compute-bound kernels.

% FIXME: why compute-bound kernels?

\subsection{Number of Dependencies}
\label{subsec:number-of-dependencies}

\input{e7_bytes}
\input{e4_radix}

The number of dependencies per task has a strong influence on the
overhead incurred by each system, as shown in
Figure~\ref{fig:radix}. This plot shows METG(50\%) for the nearest
dependence pattern, when varying the number of dependencies per task
from 0 to 9.

% FIXME: make sure these numbers are up to date for final paper

The ratio in METG between 0 and 3 dependencies per task ranges from
$1.3\times$ to $59\times$ (median $3.0\times$). From 1 to 3
dependencies, the ratio varies from $1.2\times$ to $15\times$ (median
$2.1\times$). The difference is most pronounced in systems that
perform runtime work inline. For example, MPI achieves an METG of 390
ns with 0 dependencies, but this rises to 4.6 \textmu{}s with 3 dependencies,
a factor of $12\times$ increase. This is unsurprising as in the case
of 0 dependencies, no \lstinline[language=C++]{MPI_Isend} calls are
issued at all, so MPI has nothing to do aside from executing kernels
as quickly as possible. For this reason, we recommend evaluating the performance of
programming systems under realistic workloads, rather than with
trivial dependencies.

\subsection{Baseline Overhead}

% Really want this to appear in the next section, but Latex forces it to be placed here to get the layout we want.
\input{e3_metg_compute}

One of the basic questions when considering different programming
systems is: How much overhead does the system add? This question is tricky to answer directly because some systems introduce
overhead \emph{inline} (i.e., by running system internal processes on
the same cores as application tasks), while other systems introduce
overhead \emph{out-of-line} (i.e., by dedicating one or more cores
solely to runtime use). Some systems, like Charm++, Realm, and Regent,
even support both configurations.

To answer this question, we use the METG metric to determine the
smallest task granularity that can be executed at a given level of
efficiency as a proxy for overhead. Figure~\ref{fig:metg-compute}
shows how METG(50\%) varies with node count for a variety of
dependence patterns supported by Task Bench. METG(50\%) is calculated
separately at each node count, to isolate runtime system behavior from
changes in communication latencies and topology when using
progressively larger portions of the machine.

We consider the following configurations of Task Bench:
Figure~\ref{fig:metg-compute-stencil} is a 1D stencil where each task
depends on 3 other tasks (including the same point in the previous
timestep). Figure~\ref{fig:metg-compute-nearest} is a pattern where
each task depends on 5 others, chosen to be as close as
possible. Figure~\ref{fig:metg-compute-spread} is a pattern where each
task depends on 5 others, spread as widely as possible. And
Figure~\ref{fig:metg-compute-4x-nearest} shows 4 identical copies of
the nearest dependence pattern executing concurrently.

We observe in the results that the baseline overheads of different
systems vary by over 5 orders of magnitude. It is worth
remembering when considering this metric that this is a \emph{minimum}
effective task granularity. Therefore applications with an average
task granularity of \emph{at least} this value can usually be expected
to execute efficiently. Typical task granularities will
generally be determined by the application domain being
considered. Most notably, for large-scale data analytics workloads, the higher METG values observed for Spark and
TensorFlow are sufficient. In contrast, for high-performance
scientific simulations, task granularities in the millisecond range
are useful, as such applications communicate (e.g., for halo
exchanges) much more frequently.

The least complicated pattern (stencil) is most favorable
to MPI, as it represents a truly bulk synchronous task graph with no
opportunity for task parallelism. For the stencil pattern, the
dominating factor is the basic overhead of executing a task, which is
minimal for MPI as the Task Bench implementation is bulk synchronous
and simply executes tasks one after another in alternation with
communication phases. In contrast, the asynchronous execution
mechanisms of the other systems are pure overhead in this scenario.

The gap between MPI and other systems shrinks as the complexity of the
communication pattern grows, and even reverses as task parallelism is
added in the form of multiple task graphs. Some of the slower systems
are omitted from comparisons with more complicated dependence
patterns, as they did not complete in reasonable time.

\subsection{Scalability}
\label{subsec:scalability}

METG is a useful metric in part because it summarizes the basic task
scheduling overhead of each programming system in a single number. This makes it
possible to evaluate METG at different node counts (shown in
Figure~\ref{fig:metg-compute}) to see how it is impacted by changes in
communication topology and latency.

% FIXME: update this when we have more data

Most systems for HPC are highly scalable,
but this is not true of all the systems included in this
evaluation. Lower is better in Figure~\ref{fig:metg-compute}, and a
flat line is ideal. Lines that rise with node count indicate less than
ideal scaling. Most notably, Spark is typically
intended for use in industrial data center applications with task
granularities measured in tens of seconds. Spark uses a centralized
controller, which limits throughput, and this is visible in the figure
as the line for Spark immediately rises with node count. Keep in mind also
that Spark is being evaluated here with a non-trivial dependence
pattern which is relatively unrepresentative of Spark's normal use
cases. Spark is more efficient with trivial parallelism, as described
in Section~\ref{subsec:number-of-dependencies}.

Implicitly parallel systems such as PaRSEC, StarPU and Regent that
rely on runtime analysis to build the DAG can suffer from
scalability bottlenecks if every node must consider the tasks
executing on all other nodes. PaRSEC DTD and StarPU mitigate this
partially by allowing the user to omit tasks that are not direct
dependencies of those executed on the current node. However, this DAG
trimming approach still requires some dynamic checks that scale with
the number of nodes and thus limit scalability~\cite{PARSEC_DTD}. Approaches based on compile-time analysis can
partially or fully mitigate this overhead. PaRSEC PTG improves over DTD 
by performing DAG expansion at compile time~\cite{PARSEC_DTD}, but
still retains dynamic checks leading to limited scalability. PaRSEC shard includes additional manual optimizations over DTD, completely eliminating these dynamic checks. Regent
uses a compile-time optimization to improve
scalability~\cite{ControlReplication17}; the increase in METG beyond 16 nodes is due to a known bug in Realm barrier migration which the Realm Task Bench implementation is able to manually work around.

% FIXME: remove parsec dtd shard if no space. 

% FIXME: talk about overhead spanning 5 orders of magnitude

\subsection{Load Imbalance}

\input{e5_imbalance}

One advantage of systems with asynchronous execution capabilities is
the ability to mitigate load imbalance, especially in the presence of
task parallelism. To quantify this effect,
Figure~\ref{fig:efficiency-imbalance} plots task granularity vs
efficiency curves under load imbalance with uniformly distributed task
durations. The task durations are generated with a deterministic
pseudo random number generator with a deterministic seed to ensure
identical task durations for all systems.

The MPI implementation of Task Bench, being bulk synchronous,
suffers the most under load imbalance. The biggest
difference is at large task granularities, where the imbalance has the
effect of putting an upper bound on MPI efficiency. At smaller task
granularities the effect shrinks and may even reverse as systems hit
their fundamental limits due to overhead.

The difference between the remaining systems is due primarily to the
scheduling behavior of the various systems. The execution of 4
simultaneous task graphs only partially mitigates the
load imbalance between tasks. Systems that provide an
additional on-node work stealing capability (such as Chapel with the distrib scheduler) see additional gains in
efficiency at large task granularities. However, the use of
work-stealing queues can also impact throughput at very small task
granularities. For example, Chapel's default (non-work-stealing) scheduler outperforms Chapel distrib at very small task granularities.

\subsection{Communication Hiding}

\input{e6_communication}

Also of interest is the ability of asynchronous systems to
hide communication latency in the presence of task
parallelism. Figure~\ref{fig:efficiency-communication} plots efficiency vs task
granularity curves for varying amounts of
communication, determined by the number
of bytes produced by each task (and therefore communicated with each
task dependency).

Asynchronous systems such as Charm++ are able to show two benefits in
these plots. First, by overlapping communication with computation,
such systems execute smaller task granularities at higher
levels of efficiency compared to the bulk synchronous MPI
implementation. Second, the asynchrony and scheduling flexibility from
executing multiple graphs also makes the curves smoother,
as spikes in latency due to interference from other jobs can be
mitigated, leading to more predictable performance, especially at
smaller message sizes.
