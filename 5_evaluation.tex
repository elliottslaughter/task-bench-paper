\section{Evaluation}
\label{sec:evaluation}

\input{e1_flops}
\input{e2_efficiency}
\input{e7_bytes}

\input{e3_metg_compute}

We present a comprehensive evaluation of our Task Bench implementations on up to 256
Haswell nodes of the Cori supercomputer~\cite{Cori}, a Cray XC40
machine. Cori Haswell nodes have 2 sockets with Intel Xeon E5-2698 v3
processors (a total of 32 physical cores per node), 128 GB RAM, and a
Cray Aries interconnect. We use GCC 7.3.0 for all Task Bench
implementations, and (where applicable) the system default MPI
implementation, Cray MPICH 7.7.3. Versions and flags for the
various systems are shown in Table~\ref{tab:flags}.

For GPU experiments we use the Piz Daint supercomputer~\cite{PizDaint}, a Cray XC50 with
one Intel Xeon E5-2690 v3 (12 physical cores) and one NVIDIA Tesla
P100 per node. We use GCC 6.2.0, Cray MPICH 7.7.2, and
CUDA 9.1.85.

\emph{Due to space constraints we omit our load balancing results, which will be included in the revision and final paper.}

\subsection{Compute Kernel Performance}
\label{subsec:peak-performance-and-efficiency}

In theory, any system should achieve peak performance if
the kernels are well-tuned and of sufficiently large granularity. In
practice, many subtle pitfalls of implementation or configuration can easily lead to poor performance. Verifying that peak performance is
achieved helps to ensure that evaluations of overhead and
efficiency are well-grounded.

Figure~\ref{fig:flops}, which is the full version of
Figure~\ref{fig:flops-mpi}, shows the FLOP/s achieved with a compute-bound
kernel with varying problem sizes (simulated by running the kernel for varying numbers of iterations). Each data point in the graph is the mean of 5 runs, with Task Bench configured to execute 1000 time steps of the stencil pattern. In the best case, we measure peak FLOP/s of
$1.26 \times 10^{12}$, which compares favorably with the officially
reported number of $1.2 \times 10^{12}$ \cite{Cori}. For the purposes
of measuring efficiency, we use our empirically determined number as
the baseline for 100\% efficiency.

Most systems achieve or nearly achieve peak FLOP/s. Some
systems reserve a number of cores (usually 1 or 2) for internal
use; these systems take a minor hit in peak FLOP/s compared
to systems that share all cores between the application and runtime. Some of the
higher-overhead systems struggle to achieve peak FLOP/s, though in most cases
the curves suggest that performance would continue to improve if we
were to run larger problem sizes. Unfortunately, the excessive
computational cost of running such tests makes this prohibitively
expensive. For example, the Spark job in
this case ran for over 6 hours.

Figure~\ref{fig:efficiency} plots efficiency (as a percentage of
peak FLOP/s) vs. task granularity. As described in Section~\ref{sec:metg}, this is
used to calculate METG(50\%). The
red, dashed line indicates 50\% efficiency.
In most cases, task granularity asymptotes prior to this point,
though some systems continue to improve at lower values. Accounting for this effect is one of the main arguments in
favor of using reasonable efficiency thresholds for METG instead of
empty tasks
(i.e., METG(0\%)). Empty tasks
reward strategies, such as devoting 100\% of
system resources to the runtime system, that make no sense for real
applications.

\subsection{Memory Kernel Performance}

Figure~\ref{fig:bytes} shows performance with a memory-bound kernel. We measure a peak memory
bandwidth of 79~GB/s, using a working set size of 0.5 GB. As discussed in Section~\ref{sec:task-bench}, the kernels are designed to keep the working set constant as the number of iterations decrease to avoid noisy, superlinear effects in the results. For comparison, the
OpenMP-enabled STREAM benchmarks~\cite{STREAM} report up to 98~GB/s.

Not all cores are required to saturate memory bandwidth, reducing the
impact of reserving cores for internal use. Most systems hit 100\% of
peak, unlike the compute-bound case.

The remaining experiments use compute-bound kernels.

% FIXME: why compute-bound kernels?

\subsection{Baseline Overhead}

One question when considering different programming
systems is: How much overhead does the system add? This question is tricky to answer directly because some systems introduce
overhead \emph{inline} (i.e., by running system internal processes on
the same cores as application tasks), while other systems introduce
overhead \emph{out-of-line} (i.e., by dedicating one or more cores
solely to runtime use). Some systems, like Charm++, Realm, and Regent,
support both configurations.

To answer this question, we use METG %% to determine the
%% smallest task granularity that can be executed at a given level of
%% efficiency
as a proxy for overhead. Figure~\ref{fig:metg-compute}
shows how METG(50\%) varies with node count for a variety of
dependence patterns supported by Task Bench. METG(50\%) is calculated
separately at each node count, to distinguish runtime system behavior from
changes in communication latencies and topology when using
progressively larger portions of the machine.

We consider the following configurations of Task Bench:
Figure~\ref{fig:metg-compute-stencil} is a 1D stencil where each task
depends on 3 other tasks (including the same point in the previous
timestep). Figure~\ref{fig:metg-compute-nearest} is a pattern where
each task depends on 5 others, chosen to be as close as
possible. Figure~\ref{fig:metg-compute-spread} is a pattern where each
task depends on 5 others, spread as widely as possible. And
Figure~\ref{fig:metg-compute-4x-nearest} shows 4 identical copies of
the nearest dependence pattern executing concurrently.

We observe that the baseline overheads of different
systems vary by over 5 orders of magnitude. It is worth
remembering when considering this metric that this is a \emph{minimum}
effective task granularity. Applications with an average
task granularity of \emph{at least} this value can usually be expected
to execute efficiently. Typical task granularities will
generally be determined by the application domain being
considered. Most notably, for large-scale data analytics workloads, the higher METG values observed for Spark and
TensorFlow are sufficient. In contrast, for high-performance
scientific simulations, task granularities in the millisecond range
are useful, as such applications communicate (e.g., for halo
exchanges) much more frequently.

The least complicated pattern (stencil) is most favorable
to MPI, as it provides no
opportunity for task parallelism. The
dominating factor in this case is the overhead of executing a task, which is
minimal for MPI as the Task Bench implementation simply executes tasks in alternation with
communication phases. The asynchronous
mechanisms of other systems are pure overhead in this scenario.
The gap between MPI and other systems shrinks as complexity grows, and even reverses as task parallelism is
added in the form of multiple task graphs.

Spark and Swift/T
are omitted from comparisons with more complicated dependencies, as the overheads of these systems require
excessive problem sizes (beyond what can be completed in 6
hours) to reach 50\% efficiency.

\subsection{Scalability}
\label{subsec:scalability}

METG summarizes system overheads in a single number. This makes it possible to
evaluate how communication topology and latency impact METG at
different node counts, as shown in Figure~\ref{fig:metg-compute}.
We find that
systems with the smallest METG on one node have roughly an order
of magnitude higher METG at 256 nodes. Increased communication latencies require significantly larger tasks to
achieve the same level of efficiency, so apparent differences in
overhead at small node counts can matter much less or not at
all at larger node counts.

Most systems for HPC are highly scalable,
but this is not true of all the systems included in this
evaluation. Lower is better in Figure~\ref{fig:metg-compute}, and
flat is ideal. Lines that rise with node count indicate less than
ideal scaling. Most notably, Spark is primarily
intended for industrial data center applications with task
granularities measured in seconds. Spark uses a centralized
controller, which limits throughput, and this is visible in the figure
as the line for Spark immediately rises with node count. Keep in mind
that Spark is being evaluated here with a non-trivial dependence
pattern that is relatively unrepresentative of Spark's normal use
cases. Spark is more efficient with trivial parallelism, as described
in Section~\ref{subsec:number-of-dependencies}.

\input{e4_radix}

PaRSEC, StarPU and Regent
rely on runtime analysis that can suffer from
scalability bottlenecks if every node must consider the tasks
executing on all other nodes. PaRSEC DTD and StarPU mitigate this
partially by allowing the user to omit tasks not directly
dependent on those executed by a given node. However, this approach still requires checks that scale with
node count and thus limit scalability~\cite{PARSEC_DTD}. Compile-time analysis can
partially or fully mitigate this overhead. PaRSEC PTG improves over DTD 
by performing DAG expansion at compile time~\cite{PARSEC_DTD}, but
retains dynamic checks that limit scalability. PaRSEC shard includes additional manual optimizations over DTD, completely eliminating these dynamic checks. Regent
uses a compile-time optimization to improve
scalability~\cite{ControlReplication17}; the increase in METG beyond 16 nodes is due to a known bug in Realm barrier migration which the Realm Task Bench implementation is able to manually work around.

\subsection{Number of Dependencies}
\label{subsec:number-of-dependencies}

The number of dependencies per task has a strong influence on
overhead, as shown in
Figure~\ref{fig:radix}. This plot shows METG(50\%) for the nearest
dependence pattern, when varying the number of dependencies per task
from 0 to 9.

% FIXME: make sure these numbers are up to date for final paper

%% From 1 to 3
%% dependencies, the ratio varies from $1.0\times$ to $150\times$ (median
%% $2.3\times$).

The ratio in METG between 0 and 3 dependencies per task ranges from
$1.01\times$ to $250\times$ (median $2.9\times$). The difference is most pronounced in systems that
perform runtime work inline. For example, MPI achieves an METG of 390
ns with 0 dependencies, but this rises to 4.6 \textmu{}s with 3 dependencies,
a $12\times$ increase. This is unsurprising, as with
0 dependencies no \lstinline[language=C++]{MPI_Isend} calls are
issued at all. Clearly, choosing a representative dependence
pattern is important when estimating the performance of a workload or
class of workloads.

\zap{
\input{e8_weak}
\input{e9_strong}
\subsection{Comparison with Weak and Strong Scaling}

Figures~\ref{fig:weak-scaling} and \ref{fig:strong-scaling} show
conventional strong and weak scaling of the stencil pattern for
comparison with the METG results in
Figure~\ref{fig:metg-compute-stencil}. At small node counts, where the
average task granularity is larger than the METG for most systems, the
performance is ideal. For several systems METG rises with node count,
and this becomes visible in the weak and strong scaling graphs as
non-ideal performance. Thus the shape of the weak and strong scaling
graphs effectively interpolates between ideal performance and the METG
curve depending on how close each system is to being limited by
overhead.
} % zap

\input{e6_communication}

\subsection{Overlapping Communication and Computation}

Also of interest is the ability to
hide communication latency in the presence of task
parallelism. Figure~\ref{fig:efficiency-communication} plots efficiency with varying amounts of
communication, determined by the number
of bytes produced by each task (and therefore communicated with each
task dependency).

Asynchronous systems such as Charm++ demonstrate two benefits in
these plots. First, by overlapping communication with computation,
such systems execute smaller task granularities at higher
levels of efficiency compared to the MPI
implementations. Second, the asynchrony and scheduling flexibility from
executing multiple graphs also makes the curves smoother,
as spikes in latency due to interference from other jobs can be
mitigated, leading to more predictable performance, especially at
smaller message sizes.

\zap{
\subsection{Load Imbalance}

\input{e5_imbalance}

One advantage of asynchronous execution is
the ability to mitigate load imbalance with little or no additional programmer effort, especially in the presence of
task parallelism. To quantify this effect,
Figure~\ref{fig:efficiency-imbalance} plots task granularity vs
efficiency curves under load imbalance where each task's duration is multiplied by a uniform random variable between [0,~1). The task durations are generated with a deterministic
pseudo random number generator with a consistent seed to ensure
identical task durations for all systems.

The MPI implementation of Task Bench, with its distinct computation and communication phases,
suffers the most under load imbalance. The biggest
difference is at large task granularities, where the imbalance
effectively puts an upper bound on MPI efficiency. At smaller task
granularities the effect shrinks and may even reverse as systems hit
their fundamental limits due to overhead.

The remaining differences are due primarily to
different scheduling behaviors. The execution of 4
simultaneous task graphs only partially mitigates the
load imbalance between tasks. Systems that provide an
additional on-node work stealing capability (such as Chapel with the distrib scheduler) see additional gains in
efficiency at large task granularities. However, the use of
work-stealing queues can also impact throughput at small task
granularities. For example, Chapel's default (non-work-stealing) scheduler outperforms Chapel distrib at very small task granularities. We do not consider Charm++ load balancers because the imbalance is \emph{non-persistent} (i.e., timestep $t$ is uncorrelated with timestep $t+1$). We leave analysis of persistent load imbalance to future work.
} % zap

\subsection{Heterogeneous Processors}

\input{e10_cuda_efficiency}

To determine the cost of scheduling tasks on GPUs, Figure~\ref{fig:cuda-efficiency} compares MPI
and MPI+CUDA on Piz Daint~\cite{PizDaint}. The CUDA
compute kernel achieves $4.759 \times 10^{12}$
FLOP/s, which is very close to the officially reported number
$4.761 \times 10^{12}$. The CPU achieves $5.726 \times 10^{11}$
FLOP/s. Note that the kernels perform different numbers of
operations as the GPU requires more work to reach peak
performance. The x-axis in Figure~\ref{fig:cuda-efficiency} is
normalized to keep FLOPs constant for a given problem
size.

Our MPI+CUDA implementation uses an offload model with data copied to and from the GPU on each step. We test
two configurations: \lstinline{w1} uses 1 task per GPU, whereas
\lstinline{w4} overdecomposes, using 4 MPI
ranks per GPU to push work to the GPU in parallel.
\lstinline{w4} achieves higher FLOP/s but
drops more rapidly at small problem sizes, due to the overhead of
scheduling $4\times$ as many CUDA kernel launches. Either way,
GPUs require more work to achieve high performance, and the overhead
of copying data dominates at small task
granularities, where CPUs achieve higher
performance. While Figure~\ref{fig:cuda-efficiency} is not couched in
terms of METG (as peak performance on CPU and GPU are very
different), the conclusion here is similar to
Section~\ref{subsec:scalability}: the cost of sending data and
tasks to the GPU imposes a floor on task granularity relative to the CPU, reducing the advantage at small task granularities of
very lightweight mechanisms such as those in MPI.
