\section{Task Bench}
\label{sec:task-bench}

% FIXME: This seems a bit abrupt but the old intro doesn't really make sense any more.
\zap{
To measure a wide variety of
systems and application scenarios, we developed a novel
parameterized benchmark called Task Bench.
} % zap

Task Bench
implementations execute a \emph{task graph}, with tasks for each point
in an \emph{iteration space} and dependencies between tasks determined
by a \emph{dependence relation}. A task $A$ that depends on another task $B$
must execute after task $B$, and receives the output data from
task $B$ as input. Each task can execute any one of a
number of kernels (compute-bound, memory-bound, etc.) and can generate
a configurable amount of output to control the amount of communication
performed with each task dependency. Thus Task Bench can explore a large space of application scenarios to
understand the performance of parallel and
distributed programming systems.

For simplicity, but without loss of generality, the iteration space in
Task Bench is constrained to be 2-dimensional, with time along
the vertical axis and parallel tasks along the
horizontal. Tasks can depend only on tasks from the immediately
preceding time step. Figure~\ref{fig:task-graphs} shows a number of sample task
graphs that can be implemented with Task Bench. Note that layout is
significant---in particular, column $i$ represents all tasks
with index $i$ in the iteration space over all the time
steps. Generally speaking each column will be
assigned to execute on a different processor core.

Dependencies between tasks are controlled by a configurable dependence
relation. The
dependence relation enumerates the set of tasks from the
previous time step each task depends on. This permits a wide variety
of dependence patterns to be implemented that are relevant to real
applications in high-performance scientific computing and data analysis: stencils,
sweeps, FFTs, trees, etc. Dependence patterns may also be
parameterized, such as picking the $K$ nearest neighbors, or $K$
distant neighbors. Table~\ref{tab:equations} show equations for the
dependence relations of the patterns in Figure~\ref{fig:task-graphs},
where $t$ is timestep, $i$ is column, and $W$ is the width of the task
graph.

\input{f5_task_graphs}
\input{t0_equations}

Despite its generality, Task Bench is easy to implement, making it
tractable to develop a suite of high-quality implementations. The central aspects of Task Bench, such as generating
task graphs and enumerating dependencies, are encapsulated in a core
library that is shared among all the Task Bench implementations. The
core library also includes implementations of the kernels, ensuring
that the kernels are identical in all systems, eliminating a potential
source of performance disparity that can be a pitfall for
implementations of mini-apps. Finally, the Task Bench core library manages the
parsing of input parameters and the display of output results,
ensuring that all implementations behave in a uniform manner and can
be scripted consistently. Because much of the functionality needed for
a Task Bench implementation is in the core library, implementations of
Task Bench are small: Our 15 Task Bench implementations range from 88
to 1500 lines, with several hundred lines being
typical. Listings~\ref{lst:compute-kernel} and \ref{lst:code-sample}
show excerpts from the Task Bench core, and the Dask implementation,
respectively. Only the code in Listing~\ref{lst:code-sample} is
implemented for each system, minimizing the work required for each additional system.

The Task Bench core library is fully
validating. Because the task graph configuration is explicitly
represented (though unmaterialized) in Task Bench, this representation
can be queried to determine exactly what dependencies a task should
expect. The output of every task in Task Bench is unique,
and all inputs are verified. An assertion is thrown if validation
fails. This level of self-checking ensures that every execution of Task Bench, if it
completes successfully, is correct. An evaluation of the performance impact of
validation showed it to be less than 3\% at the smallest task
granularities in any Task Bench implementation, with a negligible
effect on overall results.

\input{l0_compute_kernel}
\input{l1_code_sample}

Task Bench provides two main kernels that can be called from tasks,
which are compute- and memory-bound, respectively. The compute-bound
kernel executes a tight loop and is hand-written using AVX2 FMA
intrinsics. The memory-bound kernel performs sequential reads and
writes over an array, and is again hand-optimized with AVX2
intrinsics. The duration of both kernels can be configured by setting
the number of iterations to execute; we use this ability to simulate
the effects of varying application problem sizes. The memory-bound
kernel is carefully written to keep the working set size constant as
the number of iterations decreases, to avoid unwanted speedups due to
cache effects.
