\section{METG}
\label{sec:metg}

\input{f1_flops_mpi}
\input{f2_efficiency_mpi}

The \emph{minimum effective task granularity}, METG(50\%), for an application $A$ is
the smallest average task granularity (i.e., task duration) such that $A$
achieves overall efficiency of at least 50\%. For example, in
compute-bound applications efficiency can be measured as the
percentage of the available FLOP/s achieved. On Cori with 1.26 TFLOP/s available per Haswell node, METG(50\%) corresponds to
the smallest task granularity achieved while maintaining at least 0.63
TFLOP/s per node. For applications not amenable to being characterized by raw FLOP/s or B/s, an application-specific measure of performance can be
used instead (e.g., mesh cells processed per second).

Figures~\ref{fig:flops-mpi} and \ref{fig:efficiency-mpi} show how METG is
calculated. The application, in this case an MPI implementation of the
Task Bench stencil pattern in Figure~\ref{fig:task-graphs-stencil}, is
run on a single Haswell node of Cori with a problem size large enough
that runtime is dominated by the application's inner loops. This confirms
that the application is properly configured and that the
efficiency metric is achievable. The problem
size is then repeatedly reduced while maintaining exactly the same hardware and software
configuration (in particular, the same number of nodes and tasks). The
expectation is that as problem size shrinks,
performance will begin to drop and eventually approach zero, as shown in Figure~\ref{fig:flops-mpi}. Systems
with lower runtime overheads maintain higher performance at smaller
problem sizes compared to systems with higher overheads.

To calculate METG, the data is replotted along axes of efficiency
(i.e., as a percentage of the peak FLOP/s achieved) and task
granularity (i.e., $\text{wall time} \times \text{num.~cores}/\text{num.~tasks}$), as shown in Figure~\ref{fig:efficiency-mpi}. Note that a \emph{task} is defined
broadly to be any continuously-executing unit of application code,
and thus it makes sense to discuss tasks even in systems
without an explicit notion of tasking, such as in MPI programs written in a bulk synchronous style. In this case, the tasks run a
compute-bound kernel included in the Task Bench implementation,
described in more detail in Section~\ref{sec:task-bench}.

In Figure~\ref{fig:efficiency-mpi},
efficiency starts at 100\%. Initially task granularity
shrinks with minimal change in efficiency. However as task
granularity shrinks further, efficiency drops more rapidly, leading
to a vertical asymptote as runtime overhead dominates useful work.

% FIXME: keep this up-to-date with the graphs

METG(50\%) is the intersection of this curve with 50\% efficiency, as
shown by the red, dashed lines in Figure~\ref{fig:flops-mpi} and
\ref{fig:efficiency-mpi}. At 50\% efficiency, MPI achieves an average
task granularity of
4.6 \textmu{}s, thus the METG(50\%) of MPI is 4.6 \textmu{}s for this configuration of
Task Bench. We use 50\% because that is generally an acceptable level
of efficiency in practice, and values above 90\% can misrepresent the
performance of some systems (see
Section~\ref{subsec:peak-performance-and-efficiency}).

\input{f3_weak_scaling_mpi}
\input{f4_strong_scaling_mpi}

METG has a well-defined
relationship with quantities of interest to application developers,
namely weak and strong scaling. Figures~\ref{fig:weak-scaling-mpi} and
\ref{fig:strong-scaling-mpi} show the weak and strong scaling behavior of the MPI Task Bench running a stencil pattern at a variety of problem sizes. In these
figures, the vertical axis is shown as wall time to emphasize the
relationship to time-to-solution, but it could equivalently be shown
as task granularity (as the number of tasks per execution is
fixed). Intuitively, at
larger problem sizes MPI is perfectly efficient. This can be seen at
the top of each figure, with flat lines when weak scaling and
ideally-sloped downward lines when strong scaling. Inefficiency begins
to appear at smaller problem sizes, towards the bottom of the graph,
where lines become more compressed. At the
very bottom, the lines compress together as running time becomes
dominated by overhead. Note that the
shapes at the bottom of the strong and weak scaling curves are identical,
and conform to the shape of the METG curve (marked by the red, dashed line).

METG therefore has a direct relationship with the smallest problem
size that can be weak scaled to a given node count with a given level
of efficiency. Using the formula for task granularity above, each run
is 32 tasks wide and 1000 timesteps long, so task granularity is wall
time divided by 1000 (since Cori has 32 cores per node). The $2^{12}$
problem size in Figure~\ref{fig:weak-scaling-mpi} scales well
initially because the task granularity of 20~\textmu{}s is greater
than the METG(50\%) of MPI at small node counts (which is about 4.6-12
\textmu{}s from 1-64 nodes) but not at higher node counts (which rises
to 28 \textmu{}s at 128 nodes and 61 \textmu{}s at 256). Similarly,
METG corresponds to the point at which strong scaling can be expected
to stop. In Figure~\ref{fig:strong-scaling-mpi} the problem size
$2^{18}$ strong scales to 64 nodes, the point at which the
scaling curve intersects METG(50\%).

The METG metric has another useful property. Because METG is measured ``in place'' (i.e.,
without changing the number of nodes or cores available to the
application), METG isolates effects that are
due to shrinking problem size from effects that are due to
increased communication and other resource issues as
progressively larger portions of the machine are used. Section~\ref{sec:evaluation} contains a comprehensive evaluation of
METG including how it changes with node count for a wide variety of
programming systems.
