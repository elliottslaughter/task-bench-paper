\section{METG}
\label{sec:metg}

\input{f1_flops_mpi}
\input{f2_efficiency_mpi}

Since Task Bench permits rapid exploration of a large space
of application scenarios, one question is how to characterize the
performance and efficiency of systems under study. As noted
above, the overheads of the systems we consider vary by more than
five orders of magnitude, making it challenging to extract useful
information from weak and strong scaling runs.

Existing studies of system efficiency
typically report \emph{tasks per second} (TPS). TPS results
are difficult to interpret and apply, because efficiency (and thus
the amount of useful work) is not constrained. With empty
tasks~\cite{LegionTracing18}, the resulting upper bound on task
scheduling throughput fails to represent useful work within a
realistic application. With non-empty tasks, since the efficiency of
the overall application is typically not reported~\cite{Canary16,
  Armstrong14}, TPS is not a measurement of runtime-limited
performance. Large tasks may be used to hide any amount of runtime
overhead, while small tasks may result in a drop in total application
throughput even as TPS increases.

We introduce \emph{minimum effective task granularity}, or METG, an
efficiency-constrained metric for runtime-limited
performance. METG(50\%) for an application $A$ is
the smallest average task granularity (i.e., task duration) such that $A$
achieves overall efficiency of at least 50\%. Note that METG is parameterized by the efficiency metric. For example, in
compute-bound applications efficiency can be measured as the
percentage of the available FLOP/s achieved. On Cori with 1.26 TFLOP/s available per Haswell node, METG(50\%) corresponds to
the smallest task granularity achieved while maintaining at least 0.63
TFLOP/s per node. However, METG is not tied to peak performance, and
in applications not amenable to being characterized in this way,
another application-specific measure of performance can be used. For
example, a simulation on a mesh might use the number of mesh cells
processed per second (i.e., total number of cells divided by wall
clock time per iteration of the main simulation loop).

The choice of 50\% is a parameter and not fundamental to METG. We use
50\% in our studies to avoid pathologies associated with lower
thresholds (see Section~\ref{subsec:peak-performance-and-efficiency}),
and also because it aligns with what we observe in practice. {\color{blue} For
example, one supercomputer center instructs users applying for
projects to run a strong scaling study and then ``select the most
parallel efficient job size,'' i.e., the largest number of nodes with
a ``ratio of benchmark speed-up vs. linear speed-up above 50\%''
\cite{CSCSReport}, which corresponds to METG(50\%).}

\input{f3_weak_scaling_mpi}
\input{f4_strong_scaling_mpi}

Figure~\ref{fig:flops-mpi} shows how METG is
measured. We run the application (MPI Task Bench) on a Cori Haswell node
with a problem size large enough that runtime is dominated by kernel
execution. This result confirms
that the application is properly configured and that the
efficiency metric is achievable. The problem
size is then repeatedly reduced while maintaining exactly the same hardware and software
configuration (in particular, the same number of nodes and tasks). The
expectation is that as problem size shrinks,
performance will begin to drop and eventually approach zero. Systems
with lower runtime overheads maintain higher performance at smaller
problem sizes compared to systems with higher overheads.

To calculate METG, the data is replotted along axes of efficiency
(i.e., as a percentage of the peak FLOP/s achieved) and task
granularity (i.e., $\text{wall time} \times \text{num.~cores}/\text{num.~tasks}$), as shown in Figure~\ref{fig:efficiency-mpi}. Note that a \emph{task} is defined
broadly to be any continuously-executing unit of application code,
and thus it makes sense to discuss tasks even in systems
with no explicit notion of tasking, such as MPI. In this case, the
tasks run the compute-bound kernel shown in
Section~\ref{sec:task-bench}.

In Figure~\ref{fig:efficiency-mpi},
efficiency starts at 100\%. Initially task granularity
shrinks with minimal change in efficiency. As tasks shrink further, efficiency drops more rapidly, approaching a vertical asymptote as overhead comes to dominate useful work.

% FIXME: keep this up-to-date with the graphs

METG(50\%) is the intersection of the curve at 50\% efficiency, as
shown by the red, dashed lines in Figures~\ref{fig:flops-mpi} and
\ref{fig:efficiency-mpi}. At 50\% efficiency, MPI achieves an average
task granularity of
4.6 \textmu{}s, thus the METG(50\%) of MPI is 4.6 \textmu{}s in this
configuration. %% We use 50\% because that is generally an acceptable level
%% of efficiency in practice, and values above 90\% can misrepresent the
%% performance of some systems (see
%% Section~\ref{subsec:peak-performance-and-efficiency}).

METG has a well-defined
relationship with quantities of interest such as
time to solution. Figures~\ref{fig:weak-scaling-mpi} and
\ref{fig:strong-scaling-mpi} show the weak and strong scaling of MPI
Task Bench running a stencil pattern at a variety of problem
sizes. {\color{blue} To show the relationship with METG, the red,
  dashed line in each graph marks the \emph{minimum effective time to
    solution} (METTS). This is computed by multiplying METG by the
  number of tasks per core (in this case 1000). METG is computed
  separately for each node count, so METTS varies at scale.}
Intuitively, at
larger problem sizes MPI is perfectly efficient. This can be seen at
the top of each figure, with flat lines when weak scaling and
ideally-sloped downward lines when strong scaling. Inefficiency begins
to appear at smaller problem sizes, towards the bottom of the graph,
where lines become more compressed. At the
very bottom, the lines compress together as running time becomes
dominated by overhead. Note that the contour of the bottom of each
graph is identical and conforms to the METTS curve.

\input{e0_flags}

METG therefore has a direct relationship with the smallest problem
size that can be weak scaled to a given node count with a given level
of efficiency. Using the formula for task granularity above, each run
is 32 tasks wide and 1000 timesteps long, so task granularity is wall
time divided by 1000 (since Cori has 32 cores per node). The $2^{12}$
problem size in Figure~\ref{fig:weak-scaling-mpi} scales well
initially because the task granularity of 20~\textmu{}s is greater
than the METG(50\%) of MPI at small node counts (which is about 4.6-12
\textmu{}s from 1-64 nodes) but not at higher node counts (which rises
to 28 \textmu{}s at 128 nodes and 61 \textmu{}s at 256). Similarly,
METG corresponds to the point at which strong scaling can be expected
to stop. In Figure~\ref{fig:strong-scaling-mpi} the problem size
$2^{18}$ strong scales to 64 nodes, the point at which the
scaling curve intersects METG(50\%).

The METG metric has another useful property. Because METG is measured ``in place'' (i.e.,
without changing the number of nodes or cores available to the
application), METG isolates effects
due to shrinking problem size from effects due to
increased communication and other resource issues as
progressively larger portions of the machine are used.
% Elliott: Do I need this sentence at all? The evaluation section is literally immediately after this.
%% Section~\ref{sec:evaluation} contains a comprehensive evaluation of
%% METG including how it changes with node count.
