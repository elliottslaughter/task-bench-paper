\section{Conclusion}
\label{sec:conclusion}

Task Bench is a new approach for evaluating the performance of
parallel and distributed programming systems. By separating the
specification of a benchmark from implementations in various
programming systems, Task Bench reduces overall developer effort to
$\mathcal{O}(m + n)$ (for $m$ benchmarks on $n$ systems) rather than
$\mathcal{O}(mn)$ as has been the case for all previous
benchmarks that we know of. This has enabled us to explore a broad space of
application scenarios and to do so with a large number of programming
systems. Our experiments have enabled the following
insights:

\begin{itemize}
\item METG for current distributed programming systems varies by over
  5 orders of magnitude.  Clearly understanding the needed task
  granularity is an important consideration in choosing a programming
  system for a new application.

\item While some systems support METG(50\%) as small as 390~ns, this applies only to trivial dependencies and small CPU-based clusters. A number of factors (nontrivial dependencies, accelerators and cluster sizes in the hundreds of nodes) raise
  the METGs that can be achieved by over an order of magnitude: 100~\textmu{}s is a reasonable bound for most applications running at scale with current technologies.

\item Systems that support asynchronous execution show benefits under
  balanced computation
  and communication, and load imbalance. However, these gains can be nullified by
  high baseline overheads.

\item Task-based systems that rely on runtime analysis for the
  discovery of parallelism can suffer from sequential bottlenecks that
  limit scaling. Existing, dynamic task pruning techniques are not
  sufficient to fully mitigate this bottleneck, while static,
  compile-time approaches are able to do so.

\item Systems for large scale data analysis require very large tasks
  (tens of seconds) to scale beyond small node counts,
  reflecting the very coarse tasks and lack of need for strong scaling
  in current workloads.

\item Task Bench has proven effective in finding performance issues
  and has lead to substantial improvements in several systems
  we study.
\end{itemize}

\zap{
We have implemented Task Bench for a broad set of programming systems
spanning large scale data analytics and HPC. However, there are
systems not represented in our evaluation that would be interesting to
consider in the future. These include GASNet~\cite{GASNET07},
Habanero~\cite{Habanero11}, Hadoop~\cite{Hadoop},
HPX~\cite{Kaiser2014}, Nimbus~\cite{Nimbus17}, OCR~\cite{OCR14},
OpenSHMEM~\cite{OpenSHMEM10}, Ray~\cite{Ray18}, and UPC~\cite{UPC99}.
} % zap

Not considered in our analysis is the impact of programming system
features on programmer productivity and performance portability. Most
applications do not operate at the absolute extreme of runtime-limited
performance, and thus may choose to trade overhead for
usability. Our study helps to quantify the performance side of that
tradeoff so that users can be better informed and developers can see
the impact that features have on the performance of their programming
systems.

\section*{Acknowledgment}

This material is based upon work supported by the U.S. Department of
Energy, Office of Science, Office of ASCR, under the contract number
DE-AC02-76SF00515, by National Science Foundation under Grant
No. ACI-1450300, and the Exascale Computing Project (17-SC-20-SC), a
collaborative effort of the U.S. Department of Energy Office of
Science and the National Nuclear Security Administration, under prime
contract DE-AC05-00OR22725, and UT Battelle subawards 4000151974 and
89233218CNA000001. Experiments on the Cori supercomputer were
supported by the National Energy Research Scientific Computing Center,
a DOE Office of Science User Facility supported by the Office of
Science of the U.S. Department of Energy under Contract
No. DE-AC02-05CH11231, and experiments on Piz Daint were supported by
the Swiss National Supercomputing Centre (CSCS) under project ID
d80. A special thanks to thank Katie Antypas for her support.

The authors would like to thank the developers of the programming
systems tested in this paper. Their support enabled us to produce
high-quality implementations of the systems under study. The
developers consulted include (alphabetically by system): Chapel:
Bradford L.~Chamberlain and Elliot Ronaghan; Charm++: Laxmikant Kalé
and Sam White; Dask: Matthew Rocklin; MPI: Samuel K.~Gutiérrez and Wei
Wu; OmpSs: Víctor López and Vicenç Beltran Querol; OpenMP: Alejandro
Duran and Jeff R.~Hammond; PaRSEC: George Bosilca and Qinglei Cao;
Realm: Seema Mirchandaney and Sean Treichler; Regent: Michael Bauer,
Wonchan Lee, Seema Mirchandaney and Elliott Slaughter; Spark: Matei
Zaharia; StarPU: Samuel Thibault; Swift/T: Justin M.~Wozniak;
TensorFlow: Jing Dong, Peter Hawkins and Mingsheng Hong; X10: David
Grove, Sara S.~Hamouda and Josh Milthorpe.
