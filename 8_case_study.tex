\section{Case Study}
\label{sec:case-study}

Task Bench is useful not only as a tool for evaluating programming
system performance, but also for discovering potential areas of
improvements. During the development of Task Bench, we identified a
number of performance issues in the underlying programming
systems. These discoveries were possible because of the flexibility of
Task Bench, and our ability to rapidly run new experiments with a
variety of application scenarios. All issues below have been reported
to and acknowledged by the respective system's developers, and are
either fixed or worked around in our experiments.

Realm and Chapel use DMA subsystems that are optimized for large
copies. Early Task Bench experiments revealed high METG values that
were diagnosed by the Realm/Chapel developers as overhead due to the
cost of scheduling small copies. Subsequent improvements in Realm and
Chapel improved small copy overheads (and thus METG) by over an order
of magnitude in the case of Realm, and by $2\times$ in the case of
Chapel. These improvements affect any application where fine-grained
tasks are used.

Further analysis of Realm efficiency indicated many overheads due to
dynamic graph construction. The Realm developers implemented a new
``subgraph'' API in response to this feedback to amortize the cost of
repeatedly constructing isomorphic task graphs. This API is used in
Task Bench to achieve further speedups in the Realm implementation.

Chapel uses a naive round-robin scheduler by default, which can lead
to unexpected load balance issues because certain language features
(such as remote array assignment) implicitly generate tasks. This
resulted in poor peak performance, even at large task granularities,
which in some cases made it impossible to measure METG (because peak
efficiency did not exceed 50\%). Chapel's other schedulers add
overhead, resulting in higher METGs. We worked around this with
\lstinline{serial}.

PaRSEC uses task pruning to improve scalability at large node
counts. Initial Task Bench results achieved less than the expected
scalability: METG was rising too quickly with node count.  The PaRSEC
developers diagnosed this as a bug in task pruning, and it was
subsequently fixed. We also implemented a version of the PaRSEC code,
\lstinline{shard}, which uses manual pruning to demonstrate that
additional gains may be possible.

Early Task Bench results for Dask revealed that the cost of scheduling
a task was $\mathcal{O}(N)$ where $N$ is the number of tasks in a task
graph, causing overall cost for a task graph with $N$ nodes to be
$\mathcal{O}(N^2)$. This issue was reported to and confirmed by the
Dask developers. As a work around, our results in the paper use a
lower-level interface which does not suffer from this asymptotic
slowdown.

Initial experiments with TensorFlow revealed that the TensorFlow Task
Bench implementation was not able to use all cores on a node (thus
making it impossible to measure METG since the efficiency was below
50\%). The TensorFlow developers diagnosed this as an issue in
constant folding. The entire Task Bench task graph was being detected
as constant, and the constant-folding pass runs on one core. As a
workaround, the developers suggested marking the task graph inputs as
non-constant so that TensorFlow's standard task scheduler could be
used.
